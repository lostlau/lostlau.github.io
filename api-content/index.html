{"posts":[{"title":"大模型的Memery","content":"\b在大模型Agent的设计中，Agent所必备的能力被抽象解构成好几个重要的模块，比如planning，tools，memory等。我个人比较喜欢用下面这张经典老图的拆解来表示： 这里将一个智能体拆解成了3个最重要的模块：感知模块、决策模块和行动模块。这篇blog主要想记录对决策模块里memory的理解思考和当前比较主流的memory机制 - mem0的paper work的记录。 前言 类比于人类的记忆系统，大模型Agent的Memory模块是其实现自主决策、长期学习和环境适应的核心组件。我喜欢将memory和knowledge分开，这儿也沿用的我的一些定义。LLM本身就是一个大型信息库+知识库，为了弥补其知识的静态性，实际构建Agent的时候通常会外接知识库，这个可能是补充专业领域知识的知识库，也可能是补充最新动态信息的知识库（当然这个也可以用过搜索引擎在&lt;工具&gt;调用模块提供）。除开这些知识信息外，我倾向于把memory用于是关于用户行为偏好的（或者自身行为偏好，但目前还不具备这一点，咱不做考虑）信息记录。 Memory的分类 当前Agent多作为chatbot或者copilot的形式出现，所以主要是以XX助手的身份来服务于一个或多个用户。所以结合这种情况与当前流行的分类方式，我把memory分成：短期记忆（STM）、长期记忆（LTM）以及用户画像。 短期记忆 储存当前Agent和用户沟通的上下文，即conversion中的每一轮对话，但是这个受限于LLM本身的上下文窗口长度（比如8K tokens）。虽然随着LLM训练技术的进步这个window length在不停的增加，但是考虑到注意力可能的分散以及推理latency，将所有的对话记录都塞给LLM肯定不是一个好的选择，也不符合人类的记忆模式（当然这一点不是一个solid的论点，但是当前直觉上我们可以先这么类比。其实不一定符合人类的模式是高效的，特别是对于构建的机器个体，这个可以参见Sutton的bitter lesson）。所以短期记忆我们可以是上文轮数的一个截断，比如近10轮；或者是按交互时间的区分，比如当天的所有对话作为短期记忆。 长期记忆 conversion中的除开短期记忆的那部分，一般是更早之前的对话，那部分对话需要被实体化的记录到外部存储中，可以近乎无限制的储存，使用的时候再通过检索机制来调用合适记忆。这儿两个核心点是如何储存与如何检索的设计： 储存涉及到关键信息的提取、信息的压缩，否则如果只是原始对话信息的储存对于使用的增益很有限（还是参考context window的限制与人类记忆系统）。时间越远的信息越容易忘记、记忆冲突需要解决机制、原始信息需要往上提炼成更精炼的记忆点才会方便后续的会回忆、等等。所以储存的更多的是summary后的历史对话信息。 检索的设计更多是依赖与储存来的，传统的基于embedding+rerank的semantic search肯定是首先能想到的高效方式；同时混合的检索方式也极大概率会被考虑，特别是当储存的设计考虑的一些预定义的category或者是关键词的时候（根据具体agent场景）。 用户画像 上面的长短期记忆是by conversation的，即在一次对话里、或处理一个任务里需要跟随的。我把跨conversation需要用到的记忆放在了这一类里面，这个更多的是by user的信息（即用户的静态信息、偏好信息等），可以在不同的对话任务里进行使用。比如用户喜欢日光，那么在旅游推荐里可以使用到这一点。比如用户提到过他是一个穆斯林，那么推荐食物的时候就有相应的侧重等等。 Paper Work 1. Cognitive Architectures for Language Agents paper链接 这是一篇比较早的系统性的结构Agent的架构，通过感知科学和符号人工智能的发展史，提出了CoALA - 一种Agent架构：一个模块化的记忆组件、一个可与内部记忆与外部环境交互的结构化的行动空间、以及一个通用的用于选择行动的决策制定过程。我们这儿主要关注其中关于memory的构建。 CoALA提出将memory分为4种类型：一种短期记忆（working memory）和三种长期记忆（episodic、semantic、procedural memory）。 working memory。工作记忆，可以简单的理解成最终进入LLM的prompt所需的元素。即prompt template中的那些动态的需填入的variable。 episodic memory。情景记忆，储存的agent历史的‘经历’：过往有做了哪些‘决定’，执行了哪些‘动作’。即交互历史，什么场景下做出了哪些反应。这些记忆可以用于指导后续agent遇到相同事件的时候应该如何反应，相关用途： 决策时检索相关经验至工作记忆，辅助推理。 将工作记忆中的新经验写入情景记忆。 semantic memory。语义记忆，储存​​智能体对世界和自身的知识​​（非时间性事实）。简单理解就是知识库（内部知识库：大模型的weights；外部知识库：向量数据库），RAG本质是从​​只读语义记忆​​中检索。\b语义记忆的进一步特性是需要动态写入新知识（如LLM推理结果），实现​​渐进式知识积累​​。 procedural memory。程序记忆，储存\b\b\bagent的工作步骤，比如大模型weights（关于行为的那部分）以及人类通过代码实现的运行机制（比如workflow）。这是最底层的，一般不会变更。 2. langmem document链接 langchain的memory工程模块。 3. mem0 paper链接 最近很火的一个开源项目。方法论上不是太复杂，它主要的亮点是工程化的实现和产品化。 方法层面，它主要采用了2个步骤extraction phase和update phase来有机的实现memory的获取和更新，以此实现长时对话和跨时对话的信息一致性。\b extraction phase。通过当前上文检索出合适的memory内容。 update phase。将ADD, UPDATE, DELETE和NOOP作为4种tool方法，通过function calling让LLM实现自主判断+选择是否更新memory。 基于不同的储存机制，种memory的构建方式：mem0和mem0-g。mem0是通过向量数据库进行memory的储存，mem0-g是通过知识图谱来进行储存的。 mem0的流程： mem0-g的流程： 4. mirix paper链接 很新的一篇paper，主要提出了将memory分成6类： Core Memory：用户画像、偏好，每次都会用的东西 Episodic Memory：temporal的信息，类似日志或者日历信息（i.e.某时间做了某事） Semantic Memory：静态知识或者事实信息 Procedural Memory：完整某个任务的步骤，储存解决问题的方法论 Resource Memory：原始文件信息（pdf,doc,png等） Knowledge Vault：用户敏感信息，如密码、地址等 ","link":"https://lostlau.github.io/post/da-mo-xing-de-memery/"},{"title":"轮盘赌？","content":"\b 51去澳门再一次玩了轮盘赌，这次输了，重新反思了之前的鞅策略，\b最终发现之前确实是一直抱着侥幸的胜利（主要是钱不够了）。 轮盘赌，是起源于法国的一种赌博游戏，原名：roulette。大家看看下图应该就懂了，肯定不陌生。 规则就是你在数字或者可下注项目上下注，放一张可以押注项的图在下面。然后游戏开始后轮盘中的小球最终落在的数字如果满足你的下注项，那么你就获胜，赔率根据获胜难易来，比如你单押某个数字，那么赢了就是赢36倍，如果你是押大小或者黑白，就是赢2倍。 游戏本身就是不均衡的，因为多了个0。所有的特殊押注项都是排除0之外的。\b所以即便不加任何的暗箱操作，庄家（赌场）长期收益都是正的。 对于游戏者，除了“凭感觉”押注外，最行之有效的系统性策略就是鞅策略，即买1:1赔率的赌注，投入1单位筹码进行，如果输了就把下注翻倍，变成2单位继续投，输了就加倍继续，直到某一次获胜（这时候累积赢的是最初下注的那1单位筹码）。然后重新以1单位进行下一轮。在资金无限的情况下，这个策略可以保证玩家的获胜，虽然每次获胜就赢1单位赌注。但是真实世界里，玩家的资金是有上限的，且赌场也会在单次下注上增加一个上限。当你“运气”不好，连输10次后，你的押注筹码已经扩展了210=10242^{10} = 1024210=1024倍了。 所以作为个人或者游戏参与者，从大数定理的角度看，长期玩都是必输的。但是我确实对这个游戏有一些奇怪的兴趣（or好感）。之前的兴趣来自于获胜的经历，失败之后的乐趣来自于： 捕捉赌博中极端情况下心理的变化 对于对抗‘不可能获胜’游戏所带来的快乐 对于【1】，我确实真实的感受到再极大风险情况下的心理失衡，导致的操作变形以及非理性。比如采用鞅策略，连续输了8轮后，你还能很坚定的继续投钱坚持么？或者是已经达到了你设定的亏损线了，你是否能够坚定的离场而不是继续下重注期待再搏一次回本。当你做完苦涩的选择后，如果再一次出乎你意料的结果出现时，你下一次又怎么抉择呢？这时候的抉择是非常苦涩的，且会很容易陷入非理性。这些只有亲身经历了，在其中才会有很真切的感受。而这些的经历我觉得是很有必要的，我其实更想说的是如何更正确的做出‘苦涩’的抉择是需要靠这些经历来‘练习’的。特别是如果你在人生中想进行投资（比如二级投资，投A股买基金；或者更进一步的你进行一级市场投资），或者即便不投资，但是我对于人生的预设是一定会出现极端事件的，在那个时候如何保持理性的做出选择也是非常重要的。我觉得类似赌轮盘的游戏就像这些‘重大’场景的心理练习场。 对于【2】，多少有些M属性了，也就像我最喜欢的游戏类型是硬核rogue-like，这种随机的关卡以及硬核的难度。同时，可以反复的玩，每次都是可能是不同的结局。对于‘对抗’也多少有点逆反的理想主义，也不多说了。\b 综上，我闲暇时间用streamlit做了一个简单的轮盘赌网页游戏放在阿里云的服务器上：link 我没事的时候会上去玩轮盘，和自己进行博弈。大家感兴趣也可以上去耍，不用注册，你给自己取一个名字就行了。最好别翻墙，用国内的网络。 ","link":"https://lostlau.github.io/post/lun-pan-du/"},{"title":"强化学习的基本概念","content":" 强化学习（Reinforcement Learning, RL）是机器学习的一个分支，它致力于让智能体（Agent）在与环境（Environment）交互的过程中学习如何做出最优的决策。强化学习的核心目标是使智能体学会在给定的环境中通过尝试和错误来最大化其累积奖励。这一学习过程涉及到智能体观察环境状态（State），根据某种策略（Policy）采取行动（Action），并接收环境给予的奖励（Reward）或惩罚，以此来调整其行为。 0.基本概念 智能体（Agent）：在强化学习框架中，智能体是执行动作以影响环境的实体。 环境（Environment）：智能体所处并与之交互的外部世界，环境根据智能体的行动反馈奖励和新的状态。 状态（State）：环境在任何给定时间点的具体情况或属性，通常表示为智能体可以观察到的环境信息的集合。 动作（Action）：智能体在给定状态下可以采取的决策或行为。 奖励（Reward）：当智能体执行动作后，环境根据动作的效果给予的反馈，奖励是驱动智能体学习的关键信号。 策略（Policy）：从状态到动作的映射，定义了在给定状态下智能体应采取的动作。策略可以是确定性的（每个状态对应一个动作）或随机性的（在状态下根据某种概率分布选择动作）。 值函数（Value Function）：评估某状态或状态-动作对好坏的函数，通常是从该点出发未来可以获得的累积奖励的期望值。 模型（Model）：环境的模型预测环境如何响应智能体的动作，即给定状态和动作后预测下一个状态和奖励。在模型无关的强化学习中，智能体无需此模型即可学习。 【学习范式】 强化学习主要包括以下几种学习范式： 模型无关的强化学习：智能体通过与环境的直接交互学习，不需要对环境的模型有任何先验知识。 基于价值：通过求解一个状态或者状态下某个动作的估值为手段，从而寻找最佳的价值函数，找到价值函数后，再提取最佳策略。代表算法包括Q学习（Q-Learning）、Sarsa、Deep Q-Networks（DQN）等。 基于策略优化：直接在策略空间中进行搜索和优化，以找到最优策略。代表算法包括Actor-Criti、策略梯度（Policy Gradient）、TRPO（Trust Region Policy Optimization）、PPO（Proximal Policy Optimization）等。 模型基的强化学习：智能体利用对环境的模型进行决策和学习。这种方法需要一个环境模型来预测下一个状态和奖励，可以是显式建模也可以是智能体自行学习的模型。 【交互过程】 强化学习的交互过程通常遵循以下步骤： 智能体观察当前环境状态。 根据当前策略，智能体选择并执行一个动作。 环境根据智能体的动作转移到新的状态，并给予相应的奖励。 智能体根据奖励和新的状态更新其策略。 重复上述过程，直到达到某个终止条件，如达到目标状态或超过最大步数。 强化学习是一个动态的、试错的学习过程，其目标是使智能体能够通过学习如何在特定环境中做出最优决策来最大化累积奖励。通过不断的实践和调整，智能体能够改善其策略，逐步提升性能。 1.RL的前世 强化学习其实是一个马尔可夫决策过程(Markov decision process，MDP)，我们先看看什么是MDP。\b讲MDP前先简单的讲一些基本知识。 【随机过程】 随机过程就是单个随机现象在时间维度上的延展。比如基础概率论中讨论的扔骰子等都是一个静态的事件，\b现在如果我是每分钟都扔一下这个骰子，想研究随着事件变化扔到点数的总和，这个就变成一个动态（和时间相关）的事件了，我们用随机过程去定义这个动态的事件。 举一个比较经典的随机过程：排队。\b比如每天咖啡店从开门开始，陆续有人来买咖啡，假如来人的速率是λ\\lambdaλ人/小时，那么t时刻前来过的人总数可以用一个Poisson过程（参数为λ\\lambdaλ）来近似。Poisson过程有一些比较好的性质，可以借助这些特性来分析这个过程中某段时间内前来买咖啡的人数的分布，以此来分析可能的排队情况。 【马尔可夫过程】 设想一个天气变化的场景： 如果昨天是晴天，那么今天是晴天的概率为0.6，是多云的概率为0.275，是雨天的概率为0.125； 如果昨天是多云，那么今天是晴天的概率为0.3，多云的概率0.5，雨天的概率0.2； 如果昨天是雨天，那么今天是晴天的概率为0.1，多云的概率0.35，雨天的额概率0.55。 那么我们可以用一个状态转移矩阵来表示这种转移的概率： 今晴今云今雨\\begin{matrix} \\qquad &amp; 今晴 &amp; 今云 &amp; 今雨 \\\\ \\end{matrix} ​今晴​今云​今雨​ 昨晴昨云昨雨[0.60.2750.1250.30.50.20.10.350.55]\\begin{matrix} 昨晴 \\\\ 昨云 \\\\ 昨雨 \\end{matrix} \\quad \\begin{bmatrix} 0.6 &amp; 0.275 &amp; 0.125\\\\ 0.3 &amp; 0.5 &amp; 0.2\\\\ 0.1 &amp; 0.35 &amp; 0.55 \\end{bmatrix} 昨晴昨云昨雨​⎣⎡​0.60.30.1​0.2750.50.35​0.1250.20.55​⎦⎤​ 天气的随事件的变化，这是一个随机过程。假设我们现在知道S0,S1,…,St−1S_0,S_1,\\dots,S_{t-1}S0​,S1​,…,St−1​的所有信息，我们想知道StS_tSt​的天气状态，那么在该场景的假设下，我们可以很容易的发现我们不需要这么多的历史信息，我们只需要St−1S_{t-1}St−1​的信息就足够了（因为不管以前出现了多少晴天雨天多云天，只要昨天是晴天，那么今天是晴天的概率就一定是0.6）：即P(St∣S0,S1,…,St−1)=P(St∣St−1)\\mathbb{P}(S_t | S_0,S_1,\\dots,S_{t-1})=\\mathbb{P}(S_t | S_{t-1})P(St​∣S0​,S1​,…,St−1​)=P(St​∣St−1​)。像这种类似的场景，如果某一个时刻的状态只取决于上一时刻的状态，那么我们称它是具有马尔可夫性质的。具有马尔可夫性质的随机过程我们称为马尔可夫过程。 【马尔可夫奖励过程】 在马尔可夫过程的基础上加入奖励函数和折扣因子，就可以得到马尔可夫奖励过程(Markov reward process，MRP)。 定义奖励函数表示某个状态sss的奖励Rt(s)R_t(s)Rt​(s)，是指转移到该状态时可以获得奖励的期望，有Rt(s)=E[Rt+1∣St=s]R_t(s)=\\mathbb{E}[R_{t+1}|S_t=s]Rt​(s)=E[Rt+1​∣St​=s] 此外，实际中，因为一个状态可以得到的奖励是持久的，所有奖励的衰减之和称为回报，可用GGG表示当下即时奖励和所有持久奖励等一切奖励的加权和(考虑到一般越往后某个状态给的回报率越低，经济学直觉：人总是偏好短期反馈），也即奖励因子或折扣因子越小，用γ\\gammaγ表示)，从而有 Gt=Rt+1+γRt+2+γ2Rt+3+…=Rt+1+γ(Rt+2+γRt+3+… )=Rt+1+γGt+1\\begin{aligned} G_t &amp; = R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\ &amp; = R_{t+1}+\\gamma(R_{t+2} + \\gamma R_{t+3} + \\dots ) \\\\ &amp; = R_{t+1}+\\gamma G_{t+1} \\end{aligned}Gt​​=Rt+1​+γRt+2​+γ2Rt+3​+…=Rt+1​+γ(Rt+2​+γRt+3​+…)=Rt+1​+γGt+1​​ 而一个状态的期望回报就称之为这个状态的价值，所有状态的价值则组成了所谓的状态价值函数，用公式表达为V(s)=E[Gt∣St=s]V(s)=\\mathbb{E}[G_t|S_t=s]V(s)=E[Gt​∣St​=s]，展开一下可得： Vt(s)=E[Gt∣St=s]=E[Rt+1+γGt+1∣St=s]=E[Rt+1∣St=s]+γE[Gt+1∣St=s]=E[Rt+1∣St=s]+γE[Vt+1(S)∣St=s]\\begin{aligned} V_t(s) &amp; = \\mathbb{E}[G_t|S_t=s] \\\\ &amp; = \\mathbb{E}[R_{t+1}+\\gamma G_{t+1} | S_t = s] \\\\ &amp; = \\mathbb{E}[R_{t+1}|S_t=s] + \\gamma \\mathbb{E}[G_{t+1}|S_t=s] \\\\ &amp; = \\mathbb{E}[R_{t+1}|S_t=s] + \\gamma \\mathbb{E}[V_{t+1}(S)|S_t=s] \\\\ \\end{aligned} Vt​(s)​=E[Gt​∣St​=s]=E[Rt+1​+γGt+1​∣St​=s]=E[Rt+1​∣St​=s]+γE[Gt+1​∣St​=s]=E[Rt+1​∣St​=s]+γE[Vt+1​(S)∣St​=s]​ 最后的等式里，前半部分为当前状态得到的即时奖励E[Rt+1∣St=s]=Rt(s)\\mathbb{E}[R_{t+1}|S_t=s]=R_t(s)E[Rt+1​∣St​=s]=Rt​(s)；后半部分表示当前状态得到的所有持久奖励γE[V(St+1∣St=s)]\\gamma \\mathbb{E}[V(S_{t+1}|S_t=s)]γE[V(St+1​∣St​=s)]，可以根据从状态s出发的转移概率得到：γ∑s′∈SP(s′∣St=s)Vt+1(s′)\\gamma \\sum_{s&#x27;\\in S} \\mathbb{P}(s&#x27;|S_t=s)V_{t+1}(s&#x27;)γ∑s′∈S​P(s′∣St​=s)Vt+1​(s′)。 所以，综合两部分，可得： Vt(s)=Rt(s)+γ∑s′∈SP(s′∣St=s)Vt+1(s′)V_t(s) = R_t(s) + \\gamma \\sum_{s&#x27;\\in S} \\mathbb{P}(s&#x27;|S_t=s)V_{t+1}(s&#x27;) Vt​(s)=Rt​(s)+γs′∈S∑​P(s′∣St​=s)Vt+1​(s′) 这就是经典的贝尔曼方程（bellman equation）。直观的理解，就是当前状态的价值等价于达到当前状态所得到的即时奖励以及下一状态价值的条件期望（已知当前状态下下一时刻达到各状态的概率乘以各状态的价值）的贴现。 贝尔曼方程的计算根据全量状态的多少复杂度会有较大区别，求解其价值函数是比较重要的部分，有几大类方法：动态规划、蒙特卡洛、时序差分方法等。 【马尔可夫决策过程】 马尔可夫决策过程（MDP，Markov decision process）是在马尔可夫奖励过程的基础上再增加了一个来自外界的刺激比如智能体的动作。在MDP中，StS_tSt​(S是状态的集合)和RtR_tRt​(R是奖励的集合)的每个可能的值出现的概率只取决于前一状态St−1S_{t-1}St−1​和前一动作At−1A_{t-1}At−1​(A是动作的集合)，并且与更早之前的状态和动作完全无关。 \b\b状态s在a动作下转移到状态s'的状态转移概率可以表示如下： Pss′a=P(St+1=s′∣St=s,At=a)=p(s′∣s,a)P_{ss&#x27;}^a = \\mathbb{P}(S_{t+1}=s&#x27;|S_t=s, A_t=a) = p(s&#x27;|s, a) Pss′a​=P(St+1​=s′∣St​=s,At​=a)=p(s′∣s,a) 假定在当前状态和动作已经确定后，其对应的奖励为r，那么加上奖励项的转移概率可以表示为： p(s′,r∣s,a)=P(St+1=s′,Rt+1=r∣St=s,At=a)p(s&#x27;,r|s,a) = \\mathbb{P}(S_{t+1}=s&#x27;, R_{t+1}=r|S_t=s, A_t=a) p(s′,r∣s,a)=P(St+1​=s′,Rt+1​=r∣St​=s,At​=a) 从而奖励函数可以表示成： R(s,a)=E[Rt+1∣St=s,At=a]=∑s′∈S∑r∈Rp(s′,r∣s,a)r\\begin{aligned} R(s,a) &amp; = \\mathbb{E}[R_{t+1}|S_t=s,A_t=a]\\\\ &amp; = \\sum_{s&#x27;\\in S}\\sum_{r\\in R}p(s&#x27;,r|s,a)r \\end{aligned} R(s,a)​=E[Rt+1​∣St​=s,At​=a]=s′∈S∑​r∈R∑​p(s′,r∣s,a)r​ 如果R是确定的，那么也等价于∑s′∈Sp(s′,r∣s,a)r\\sum_{s&#x27; \\in S}p(s&#x27;,r|s,a)r∑s′∈S​p(s′,r∣s,a)r。 至于过程采取什么样的动作就涉及到策略policy，策略函数一般用π\\piπ表示，把状态映射到动作：a=π(s)a=\\pi(s)a=π(s)或者a=πθ(s)a=\\pi_{\\theta}(s)a=πθ​(s)（其中θ\\thetaθ表示策略函数的参数）。不过更常见的情况是状态映射到动作也是随机的，某种状态下有多少概率采取某种动作，所以π\\piπ在这种情况下用来表示概率测度，即π(a∣s)=P(At=a∣St=s)\\pi(a|s) = \\mathbb{P}(A_t=a|S_t=s)π(a∣s)=P(At​=a∣St​=s)\b。 引入动作之后我们重新来梳理一下价值函数 首先通过状态价值函数对当前状态进行估计 Vπ(s)=Eπ[Gt∣St=s]=Eπ[Rt+1+γGt+1∣St=s]=Eπ[Rt+1+γVπ(St+1)∣St=s]\\begin{aligned} V_{\\pi}(s) &amp;= \\mathbb{E}_{\\pi}[G_t|S_t=s]\\\\ &amp;=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s]\\\\ &amp;=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma V_{\\pi}(S_{t+1})|S_t=s] \\end{aligned} Vπ​(s)​=Eπ​[Gt​∣St​=s]=Eπ​[Rt+1​+γGt+1​∣St​=s]=Eπ​[Rt+1​+γVπ​(St+1​)∣St​=s]​ 然后通过动作价值函数对动作评估 Qπ(s,a)=Eπ[Gt∣St=s,At=a]=Eπ[Rt+1+γGt+1∣St=s,At=a]=Eπ[Rt+1+γQπ(St+1,At+1)∣St=s,At=a]\\begin{aligned} Q_{\\pi}(s,a) &amp;= \\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\\\\ &amp;=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a]\\\\ &amp;=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma Q_{\\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a] \\end{aligned} Qπ​(s,a)​=Eπ​[Gt​∣St​=s,At​=a]=Eπ​[Rt+1​+γGt+1​∣St​=s,At​=a]=Eπ​[Rt+1​+γQπ​(St+1​,At+1​)∣St​=s,At​=a]​ 的价值函数可以扩展成动作价值函数Qπ(s,a)Q_{\\pi}(s,a)Qπ​(s,a)，相当于对当前状态s依据策略π\\piπ执行动作a得到的期望回报，这也是大名鼎鼎的Q函数 1.Q-learning Q-learning是一种无模型的强化学习算法，主要用于学习在给定状态下采取各种动作的预期效用。它通过估计一个动作价值函数，即Q函数（Q-value function），来指导智能体如何在给定状态下选择动作以最大化累积奖励。Q函数的值Q(s, a)表示在状态s下采取动作a的长期回报的预期值。 原理 Q-learning的核心是更新Q值的规则，它使用贝尔曼方程（Bellman equation）的一种变体来迭代更新Q值。该更新规则如下： Q(s,a)←Q(s,a)+α[r+γmax⁡a′Q(s′,a′)−Q(s,a)]Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a&#x27;} Q(s&#x27;, a&#x27;) - Q(s, a)] Q(s,a)←Q(s,a)+α[r+γa′max​Q(s′,a′)−Q(s,a)] 其中： Q(s,a)Q(s, a)Q(s,a) 是当前状态sss下采取动作( a )的Q值。 α\\alphaα 是学习率（0 &lt; α\\alphaα≤ 1），控制学习过程中新信息的重要性。 rrr 是采取动作( a )后从状态( s )转移到状态( s' )获得的即时奖励。 γ\\gammaγ 是折扣因子（0 ≤ γ\\gammaγ &lt; 1），表示未来奖励的当前价值。 max⁡a′Q(s′,a′)\\max_{a&#x27;} Q(s&#x27;, a&#x27;)maxa′​Q(s′,a′) 是在下一个状态s′s&#x27;s′下所有可能动作的Q值中的最大值，代表未来奖励的最佳估计。 技术细节 探索与利用（Exploration and Exploitation）：为了平衡探索新动作和利用已知最佳动作之间的关系，Q-learning通常采用ε-贪婪策略（ε-greedy policy）。这意味着大部分时间（1-ε的概率）选择当前已知的最佳动作（最大Q值的动作），而有一小部分时间（ε的概率）随机选择动作，以探索未知的状态-动作对。 学习率（Learning Rate）：学习率α\\alphaα决定了新信息替代旧信息的速度。较高的学习率意味着新的估计会更快地覆盖旧的估计，而较低的学习率意味着学习过程更加缓慢，更多地依赖历史数据。 折扣因子（Discount Factor）：折扣因子γ\\gammaγ决定了未来奖励对当前决策的重要性。当γ\\gammaγ接近1时，智能体会更加重视长期奖励；当γ\\gammaγ较小时，智能体则倾向于优先考虑短期奖励。 初始化Q值：Q-learning算法的性能部分依赖于Q值的初始化。通常，Q值可以初始化为0或一个小的随机值，以促进初期的探索。 策略收敛：理论上，如果每个状态-动作对被无限次访问，且学习率符合特定条件（如随时间逐渐减小），Q-learning保证能够找到最优策略。 函数逼近：在具有大量状态或连续状态空间的环境中，直接使用表格存储Q值变得不可行。在这种情况下，可以使用函数逼近方法（如神经网络）来估计Q函数，这是深度Q网络（DQN）的基础。 Q-learning算法因其简单性和在多种环境中的有效性而广受欢迎。尽管它在处理高维状态空间和连续动作空间时面临挑战，但通过与深度学习等技术的结合，Q-learning及其变种已经在诸如游戏、机器人导航等领域取得了显著的成功。 2.Sarsa Sarsa（State-Action-Reward-State-Action）是一种在强化学习中使用的无模型的策略迭代算法，与Q-learning相似，它采取行动的时候用的也是ε-greedy policy，但主要区别在于它是一种同策略（on-policy）学习方法。这意味着Sarsa在更新值函数时使用的是实际会产生的下一步动作，而不是像Q-learning中那样使用最大化未来奖励的动作（理论动作）。和Q-learning的唯一区别就是在更新Q函数的时候，采用的规则为： Q(s,a)←Q(s,a)+α[r+γQ(s′,a′)−Q(s,a)]Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\textcolor{red}{Q(s&#x27;, a&#x27;)} - Q(s, a)] Q(s,a)←Q(s,a)+α[r+γQ(s′,a′)−Q(s,a)] 红色部分为不同的地方，这儿a′a&#x27;a′的选择不再仅仅是根据Q来了（取Q最大的），而是完全遵守当前策略（Policy）来得到的。 区别是什么？其实就是在ε-greedy上，完全遵守当前策略的话，那么就是有ε的可能性是完全随机的，而（1-ε）下是根据当前Q函数取了最大的那个action。所以Sarsa的Q函数更新过程是“知行合一”的，我在“确定了”下一步action后再更新行为价值，比如下一步可能由于ε-greedy落到了随机选择的那个a′a&#x27;a′。而Q-learning则是在“推演”之后的行为和行为价值的过程中仅仅设想了我“理性的”会选择Q最大的那一个来确定之后的行为价值参照（更新后的Q函数），而真实的下一步行动则是在更新的Q函数上采用当前Policy（i.e. ε-greedy）来制定的，所以真实的a′a&#x27;a′和在进行Q更新时候的所设想的a′a&#x27;a′，即arg max⁡a′Q(s′,a′)\\argmax_{a&#x27;} Q(s&#x27;, a&#x27;)a′argmax​Q(s′,a′)，可能是不一样的。为了区别这两种更新策略，定义了on-policy（目标策略=行为策略，比如Sarsa）和off-policy（目标策略!=行为策略，比如Q-learning）。 相比Q-learning，Sarsa更谨慎保守，因为Q-learning在Q函数的更新上总是朝收益最大的action倾斜，以此来直到实际的行为策略；而Sarsa更新Q函数的时候已经考虑了可能的随机探索情况，所以不会这么激进。 ","link":"https://lostlau.github.io/post/intro_reinforcement_learning/"},{"title":"从attention到GPT","content":"去年搞了半年的大语言模型的应用开发。大语言模型的应用技术门槛相对较低，不过发挥第一性原理，抽点时间再回顾一下跟模型架构相关的理论知识。 简略的一些纪要，不会做详细的讲解。 0.故事线 现在开源大模型的框架（Llama、GLM、Qwen等）大多是基于GPT的变种，所以GPT的架构到底长什么样？虽然GPT脱颖而出了，但是其同时期的经典框架和它的区别又是什么？要知道这些，需要往下看一层：transformer（因为这些都是基于transformer的上层建筑），所以transformer又长什么样子？我们再往下看，transformer里最核心的结构是attention，它长什么样子？优势又是什么？同时transformer又是一种seq2seq架构，那么什么又是seq2seq？基于这些追问，我们反过来重头开始看。 1. Seq2Seq Seq2Seq（Sequence to Sequence）是一种在自然语言处理（NLP）领域常用的模型架构，尤其在机器翻译、文本摘要、问答系统和对话系统中非常流行。Seq2Seq模型的核心目标是将一个序列转换为另一个序列，这两个序列的长度可以不同，这使得它非常适用于上述任务。该架构最初由Ilya Sutskever, Oriol Vinyals和Quoc V. Le在2014年提出（原文arxiv地址），主要目的是解决当时DNN不能很好的处理输出长度不一致的监督学习任务。该架构定义两个核心组成部分：编码器和解码器。 编码器（encoder）：编码器的任务是接受输入序列，并将其转换为固定大小的上下文向量（或称为“状态向量”）。这个上下文向量被认为是输入序列的内部表示，包含了输入数据的关键信息。 解码器（decoder）：解码器的任务是接受编码器产生的上下文向量，并基于这个向量生成输出序列。在开始生成过程时，解码器通常接受一个特殊的起始符号，以指示开始生成序列。然后，它逐步生成输出序列中的每个元素，直到遇到特殊的结束符号(&lt;EOS&gt;)，表示序列生成完成。 可以想像成encoder主要做表示学习，把原始input表示成高维抽象特征并且学习其空间分布（固定维度）；然后decoder进行自回归生成（采用beam search，解释见下），基于encoder的表征做预测。Ilya的原文中encoder和decoder都是用的LSTM。同时，原文还用了将input倒置的trick（比如 正常句子是abc，但是先颠倒成cba作为模型的input），目的是减少长期依赖的距离。后来有了attention的使用，解决了传统RNN的长时记忆问题，所以这种trick现在基本不使用了。 【beam search】 Beam Search 是一种启发式图搜索算法，常用于自然语言处理中的序列生成任务，特别是在使用序列到序列（Seq2Seq）模型进行机器翻译、文本摘要、语音识别等任务时。与简单的贪心搜索不同，它在每一步并不只是选择概率最高的一个词，而是保留多个最有可能的候选选项，这些候选选项被称为beam。比如目标是最大化： P(y1,…,yT′∣x1,…,xT)=∏t=1T′P(yt∣v,y1,…,yt−1)\\mathbb{P}(y_1,\\dots,y_{T^{&#x27;}}|x_1,\\dots,x_T)=\\prod_{t=1}^{T^{&#x27;}} \\mathbb{P}(y_t|v,y_1,\\dots,y_{t-1}) P(y1​,…,yT′​∣x1​,…,xT​)=t=1∏T′​P(yt​∣v,y1​,…,yt−1​) 其中vvv是当前的表征空间（状态特征）。为了最大化左式，可以采用贪心算法（也是beam=1的情况），即在右式中每一个t的时候，都取当前条件概率最大的那个yty_tyt​。但是这样很可能只能得到局部最优，但是计算效率较高。在此基础上我们可以牺牲一点计算效率，然后最优更优的可能性：比如在每一步t，保留可能组合中的top2高的条件概率对应的yt(1)y_t^{(1)}yt(1)​和yt(2)y_t^{(2)}yt(2)​，直到&lt;EOS&gt;(停止符，end of sentences)这便是beam=2的情况。相同的，可以每一步都保留n个当前概率最高的可能情况，就是beam=n。所以beam趋于∞\\infty∞的时候，就是全空间搜索，必定得到全局最优，但是相应的计算效率就很低了。 2. Attention 通俗的说，attention机制是模拟人类注意力聚焦特性而设计的一种计算单元，它允许模型在处理数据时“关注”到序列的某些部分更多一些，而不是平等的对待每个元素。设想你阅读一本书你的大脑并不是平等地关注每一个单词。相反，你会根据当前句子的上下文和你想要理解的内容，集中注意力于某些词语上。在传统的序列处理模型中，模型是将所有信息压缩进一个固定大小的向量中，不管序列有多长，这就好比要你在听完一个长故事后，只用一个句子来复述它，这显然很难做到既准确又全面。而attention允许模型在生成每个输出时“回头看”输入序列的不同部分，从而根据需要关注更相关的信息。这就像在复述故事的每个部分时，都能重新参考原文，确保你的复述既准确又相关。 从技术层面来讲，注意力机制可以被看作是一个加权求和的过程，其中加权系数决定了在生成输出时对输入序列的不同部分的“关注”程度。这个机制通常被集成到序列处理模型中，不止transformer，像RNN、LSTM也都能使用这个。注意力机制的核心是一个可学习的权重分配过程，这个过程会为输入序列中的每个元素（如单词或字符）分配一个权重，表明每个元素对当前输出元素（如翻译中的单词）的重要性。这些权重随后被用来计算加权平均的上下文向量，该向量将被用作生成当前输出的额外输入。 假设我们有一个输入序列X={x1,x2,…,xn}X=\\{x_1,x_2,\\dots,x_n\\}X={x1​,x2​,…,xn​}和一个当前的query（比如decoder中生成的上一状态或输出），attention的计算可以分为以下几步： query, key, value：首先，对于序列中的每个元素xix_ixi​，我们通过可学习的参数转换它们为key(kik_iki​）和value（viv_ivi​）。同时，当前的query（qqq）也通过相似的变换得到。 注意力得分的计算 ：然后，我们计算qqq与每个kik_iki​之间的相似度，即注意力得分a(q,ki)a(q,k_i)a(q,ki​)。这个得分经典的通过点积得到q⊤ki/dq^{\\top}k_i/\\sqrt{d}q⊤ki​/d​（其中ddd是query的维度）。不过注意力得分也有其他的设计，比如加性得分tanh(q+ki)tanh(q+k_i)tanh(q+ki​)等。 归一化：注意力分数通过softmax函数进行归一化，以确保所有的权重加起来等于1。这些归一化的分数称为注意力权重aia_iai​。 ai=exp⁡(a(q,ki))∑j=1nexp⁡(a(q,ki))a_i=\\frac{\\exp(a(q,k_i))}{\\sum_{j=1}^n\\exp(a(q,k_i))} ai​=∑j=1n​exp(a(q,ki​))exp(a(q,ki​))​ 加权平均：最后，计算value（vi,∀i=1,…,nv_i, \\forall i=1,\\dots,nvi​,∀i=1,…,n）的加权平均，权重为对应的注意力权重aia_iai​。得到的加权和被称为上下文向量ccc，它是输入序列的加权表示，突出了对当前输出最重要的部分。 c=∑i=1nai⋅vic=\\sum_{i=1}^n a_i·v_i c=i=1∑n​ai​⋅vi​ 下面左图是通用的attention机制，右图是经典的点积attention。 attention里query（QQQ）、key（KKK）和value（VVV）可以是完全不同的3个特征向量。对某个query（qqq）进行相似度查询，查询对象列表就是整个key列表QQQ，检索到相关的key后映射到其value，然后做一个聚合（加权求和）。有点像比如菜市场里所有菜的名字和价格构成了KKK和VVV，然后你这次想做一个番茄炒蛋，需要买菜，那么你带着这个目的去菜市场，番茄炒蛋就是本次的qqq。先q和K进行相似度检索，那么番茄炒蛋最需要的是番茄、鸡蛋，这两个权重很大（相似度得分高）；然后还需要可能洋葱、小葱或者其他的配菜（相似度得分稍低）；那么对于菜市场里有的其他的比如猪肉、鱼这种完全用不到的，在这次买菜（query）里相似度得分就接近0。相似度检索完后找到需要买的菜的价格（VVV），然后按权重计算本次买菜行为需要的成本就可以。 【self-attention】 attention机制中有一个很特殊的情况，就是Q=K=VQ=K=VQ=K=V，一个输入同时承担了三种角色，这就是自注意力机制。在NLP任务中，所说的attention基本都是self-attention，因为这些任务（比如机器翻译）中一般输入都只有一个东西，就是句子转换成的词向量组。它们自己和自己玩儿，自我探索结构词和句子语义间的关系😄。 【multi-head attention】 简单的讲就是多个attention的联合。在实际中，给定一组QKV，我们肯定通过注意机制可以捕捉更多的知识，例如捕获这个序列中各种范围的依赖关系（较短范围与较长范围）。因此，直觉上让注意力机制联合使用QKV的不同表示子空间可能是更好的方式，就是说创建多个attention，然后最后再concat起来。 在实际操作中，multi-head attention的实现一般会使用一个大的组合权重张量来代表所有的head，执行一次大的矩阵乘法，然后将其分割成每个head的Q,K,VQ,K,VQ,K,V矩阵。从理论上讲，这不影响模型输出，因为代数运算是相同的。 3.transformer 最原始的Seq2Seq还是基于RNN的，其不足： encoding过程中长距离衰减问题，当句子过长，越往后的hidden state蕴含的前面的信息就会越弱。 同样的，decoding过程也是一样的长距离衰减问题 解码阶段缺乏对编码阶段各个词的直接利用。简单说就是：机器翻译领域，解码阶段的词和编码阶段的词有很强的映射关系，比如“爱”和“love”。但是seq2seq模型无法再译“love”时直接使用“爱”这个词的信息，因为在编码阶段只能使用全局信息。 attention的提出极大的改善了上面的问题，不过attention有个问题是没考虑顺序（这个带来另一个优势就是可以并行，不用像RNN只能串行）。 2017年Google发布了transformer架构，基于Seq2Seq（Encoder-Decoder），并且把RNN踢出了局，改为只用（Self-）Attention+FFNN（前馈神经网络）作为encoder、decoder的核心。所以这篇文章叫：Attention is all your need。 上一张经典图来展示其架构： 其序列位置信息的纳入是通过positional encoding的方式来进行的（解释见下面对应部分）。左边灰色框（self-attention + FFNN）是encoder，右边灰色框（masked self-attention + cross attention + FFNN）是decoder。原始transformer里encoder和decoder分别要堆叠6层（有点类似于从单层NN到深度NN，6只是个超参数）。encoder是一个AE（autoencoding）模型，decoder是一个AR（autoregressive）模型。encoder进行的是上下文的理解，复杂特征空间表示学习；而相应的decoder进行一个一个按顺序的预测生成。所以可以看到encoder都是self-attention，到了decoder里的第二层是cross-attention，就是把encoder里学到的表示作为key和value，然后将第一层里的结果作为query。decoder里第一层里使用了mask掩码技巧（解释见下面对应部分）。 【positional encoding】 原文中用的是这种方法，使用函数f:N→Rdf:\\mathbb{N} \\rightarrow \\mathbb{R}^df:N→Rd，直接将位置索引值映射到ddd维向量上。公式为： f(p,i)=1{i为偶数}⋅sin⁡(p10000id)+1{i为奇数}⋅cos⁡(p10000i−1d)f(p,i)=\\bold{1}_{\\{i为偶数\\}}·\\sin(\\frac{p}{10000^{\\frac{i}{d}}})+\\bold{1}_{\\{i为奇数\\}}·\\cos(\\frac{p}{10000^{\\frac{i-1}{d}}}) f(p,i)=1{i为偶数}​⋅sin(10000di​p​)+1{i为奇数}​⋅cos(10000di−1​p​) 其中ppp为序列中位置索引值，0≤i&lt;d0\\leq i&lt;d0≤i&lt;d是位置编码向量中的位置索引。比如在原序列中的第2个位置就被编码成（假设位置向量是10维）：[f(2,0),f(2,1),…,f(2,9)][f(2,0), f(2,1), \\dots, f(2,9)][f(2,0),f(2,1),…,f(2,9)] 除了上述的编码方法，还可以使用embedding的手段，就是说像word embedding一样，为序列中每个绝对位置也赋予一个连续、低维、稠密的向量表示。 【FFNN】 这儿前馈神经网络由两个线性变换（全连接层）和一个非线性激活函数组成： FFN(x)=max⁡(0,x×W1+b1)×W2+b2\\mathrm{FFN}(x) = \\max(0, x\\times W_1+b_1) \\times W_2 + b_2 FFN(x)=max(0,x×W1​+b1​)×W2​+b2​ 【masked self-attention】 在decoder里的第一层，目的是在训练的时候不使用未来信息，因为attention可以并行，训练时可以同时进行query和key的点积计算，不过在一句话中当前位置的词query只能跟该位置之前的词key进行注意力得分计算（而在encoder里也可以和这个位置之后的词进行，因为encoder是在做语义的理解）。之后mask的技巧也衍生出不同的掩码方式，这种最原始只向左看齐就是causal mask，此外还有prefix mask等。 4.Pre-trained Models 经典的NLP领域预训练模型也不是照搬的transformer结构，参考这篇blog分为了以下3种流派：encoder-only（AE），decoder-only（AR）和encoder-decoder（Seq2Seq）。见下图： decoder-only优缺点： 优势： 适合生成式任务。因为AR模型使用causal attention去预测下一个token，训练目标也更多以预测下一个词为主，所以天然的就更适合生成任务。 劣势： 只能适用于单向（正向或者逆向）上/下文预测任务，不适用于需要双向上下文的任务。 encoder-only优缺点： AE模型更多是用于内容理解的任务，比如经典的BERT（填补缺失句子中的词信息+预测两个句子间是不是前后关系）。 优势： 可以同时用到上下文的信息，对词与词，句子与句子间的理解捕捉得更好。 劣势： 训练中会用到掩码标记[mask]，然而在真实的预测任务中不存在 独立性假设，假设了[mask]和[mask]之间是相互独立的，这个和真实的情况也是有偏差的（同一个句子中这个mask已知之后会改变下一个mask预测的条件分布） encoder-decoder： 同时兼具了AE和AR模型，这种通常用于处理一些需要在输入和输出间建立精确映射的任务，比如机器翻译、文本摘要等。在这些任务中，理解输入的精确内容并据此生成特定的输出是非常重要的。而基于这种架构训练出来的模型，一般只能应用于某种特定的任务。 从这3种架构流派看，encoder-only主打一个理解，decoder主打生成，encoder-decoder从理论上讲应该是最完美的。但是从实践来看，当前主流的LLM都是decoder-only框架，其原因也给一些主观的观点： decoder-only更“知行合一”，他的训练和预测的任务是一致的，更容易往“one for all”的去走，无论什么下游任务，都变成根据指令的生成任务（根据指令的自回归的next word prediction），通过SFT+RLHF来训练。 encoder-only本身设计就是要搭配fine-tuning的，没办法很好的one for all（多个下游模型的fusion空间的对齐也很困难，至少对于当前来说成本很大）。encoder-decoder的精确度是最高的，同时也带来的弊端就是它只适用于specific的任务，它需要被泛化到通用的one for all的代价就更高。 想说timing这个点，其实也是成本导向，decoder-only逻辑上就是最容易先出成果的方向，其到one for all程度的应用实践所需的训练数据的获取、处理、训练时间都比encoder+FT框架更容易、更快。所以当前看decoder-only框架好像更好，可能也只是当下，decoder-only进入瓶颈期后说不定encoder类的框架也会卷出新的高度。当然也可能是跳出transformer的框架。世界总不缺创新的探险者。 【causal attention】 causal的意思是因果(掩码)，也就是每个tokens只能关注到自己和自己左侧的tokens。因果两个字就说明了只能从前到后，意思是第二个字的表示应该包含第一个字的贡献，相反，第一个字的表示，不应该受到第二个字的影响，这就是因果，只有过去可以影响现在，现在不能再对过去产生影响。causal attention也就是带因果掩码的attention机制。 还有一种是prefix attention，其实也是causal的，相当于是对于已经吐出来的那部分subsentence已经是可以相互之间捕捉相关性的了（比如下图中的x1,x2,x3x_1,x_2,x_3x1​,x2​,x3​）。\b 与这些相对的是全部都可视的，一般在encoder里存在，相当于做语义理解，捕捉词与词，词与句子的关系，做这些表示学习的时候都是全可见的。 见下图（来自于paper：Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer）： 4.1. BERT pass 4.2. GPT 具体可以看这篇blog，讲得很好。我就只总结以下重点。 GPT3使用了Sparse Attention（见下），提高计算效率。 自从GPT2开始，decoder的layer normalization位置提前了，原生的layer normalization在每次Attention 或者 FeedForward之后，GPT2提到了之前。然后在最后一次decoder之后，进入最后的Linear+Softmax之前增加了一词layer noramlization。 decoder有12个。 每个decoder里multi-head attention有96个head。 最后贴一张blog里的手绘架构图，很强（可能看不清楚，建议下下来看或者去原文里看）。 【Sparse Attention】 最原始的GPT使用了sparse attention，这个是将attention进行了一些改造，主要是提高计算效率（会降低一些理论精度）。大概看一下我之前的这篇也有讲过这个。 推荐一篇很好的可视化blog：LLM visualization，用其中nano-gpt的例子很好的可视化讲解了类GPT的核心结构。 5. LLM \b回到大语言模型上，\b截止目前，大部分的LLM都是采用decoder-only的transformer架构基础，然后在这个基础上做一些变化。这儿再展开讲一下decoder-only的核心架构，它是在原始transformer架构上，移除了以下两个部分： 整个encoder部分 decoder中的encoder-decoder self-attention层。 大语言模型（或者更广义的基础模型）的提出动机来自于两个事实： 大量丰富的非标注语料 标注数据的稀缺性 同时，传统大多数深度学习模型，需要大量的标注数据去解决一个表现良好的判别任务，表现得更显一个“局部领域的专家”。它们有一些很难被逾越的缺陷： 一些领域没有足够的标注数据。 每次新任务都必须重新建模，成本高。 与其每次去训练一个新模型，不如先预训练一个语言模型，然后在此模型的基础上通过一些方式去适配解决各种下游的语言任务。一种方式就是fine-tuning在更多数据上训练一个适合下游任务的模型；另一种更突破性的，就是使用零样本（zero-shot）/少样本（few-shot）推理。 零样本/少样本推理的过程被叫做prompting，其实就是通过指令让语言模型给出回答（这些指令被叫做prompt）。在训练GPT类模型的时候，其接收到的文本数据input一般都是类似于如下样子： &quot;Translate this sentence to English: =&gt;&quot; &quot;Summarize the following document: =&gt;&quot; 这些以解决任务的指令input在大规模语料的学习下拥有了zero-shot（在没看过正确output的前提下也能进行作答）/few-shot（给定一些指令和对应指令的正确回答的例子然后进行作答）推理的能力。 要较好的实现以上的zero-shot/few-shot的能力，除了架构和数据，还有工程化很重要的一点：训练方式或者训练步骤。 Tokenization Text Embedding Pre-training supervised fine-tuning（SFT） reinforcement learning with human feedba（RLHF） 下面对一些经典的LLM家族的架构进行说明，主要以推理视角，所以以架构为主；训练视角的元素，如训练方式和训练数据如无需要不做特别的说明。（ 有时间将会持续更新。） 5.1. Llama Llama基本采用了传统decoder-ony transformer的架构，但在以下3个地方进行的修改： Normalization。layer normaliztion改成了root mean square normalization（RMSNorm for short），这是一个简化版的LayerNorm，就是将LayerNorm里减去均值的操作省略了（所有xi−xˉx_i-\\bar{x}xi​−xˉ的地方都变成了xix_ixi​），同时还将线性转换中的需要学习的偏置项取消掉，公式：y=xixˉ2+ϵ×Wy = \\frac{x_i}{\\sqrt{\\bar{x}^2+\\epsilon}}\\times Wy=xˉ2+ϵ​xi​​×W。实验验证以上简化后速度提升40%，效果几乎无损。此外，Llama还采用了pre-nromalization的设计，如GPT\b2里使用的一样，将normalization处理时机提前，如下图： Activation Function。Llama采用了SwiGLU替代传统的ReLU作为Feed Forward层的激活函数，这个激活函数使用门控单元，里面的包含了2个线性变换，所涉及的权重也是需要学习的，这是和一般的激活函数不同的地方。 SwiGLU=Swish(xWg+bg)⊗x(W+b)Swish(x)=x⊗Sigmoid(βx)\\begin{aligned} &amp;\\mathtt{SwiGLU}=\\mathtt{Swish}(xW_g+b_g)\\otimes x(W+b) \\\\ &amp;\\mathtt{Swish}(x)=x\\otimes \\mathtt{Sigmoid}(\\beta x) \\\\ \\end{aligned}​SwiGLU=Swish(xWg​+bg​)⊗x(W+b)Swish(x)=x⊗Sigmoid(βx)​ 采用SwiGLU激活的FFN可以表示成（no-bias形式）： FFNSwiGLU(x)=(Swish1(xW)⊗xV)W2\\mathrm{FFN_{SwiGLU}}(x) = (\\mathrm{Swish}_1(xW)\\otimes xV)W_2 FFNSwiGLU​(x)=(Swish1​(xW)⊗xV)W2​ RoPE。在位置编码（positional embedding）上引用Rotary Positional Embedding，是一种结合了绝对位置编码和相对位置编码的技术。具体细节见这篇blog或者这篇知乎。使用RoPE可以在长序列上带来更好的位置嵌入效果。需要补充的是，RoPE是和self-attention同时进行的，所以不是跟传统的PE一样在Input Text Embedding的时候进行一次，而是在每个有self-attention的地方都需要运行。 在以上Llama的变化基础上，Llama还对架构进行了进一步修改，最关键就是采用了grouped-query attention来替代原始的multi-head attention。grouped-query attention介于multi-head attention和multi-query attention（由这篇paper提出，在PaLM模型里使用并得到较好实践效果）之间，3者关系如下图所示： 其中，multi-query attention的改变是在于在每个head之间共享了相同的K,VK,VK,V，这使得推理的速度可以得到明显提升（以前K,VK,VK,V上计算HHH次的现在只需要1次）。\b\b但是提一下，训练速度上并无太大提升，原因是训练更大的开销在于梯度下降的计算和参数更新上。可见，grouped-query attention是将原始的HHH个multi-head又分成了h(&lt;H)h(&lt;H)h(&lt;H)个小组，然后在小组间共享K,VK,VK,V，相当于是在预测效果和推理速度之间做了一个trade-off。 此外，在多轮对话推理的优化上使用了Ghost Attention技术。 5.2. Falcon \bFalcon依然采用的decoder-only causal transformer 架构。在传统基础上，进行了如下几处修改： Flash Attention。这是一个可以加速attention计算的算法优化，不过这个算法的核心不是在理论上降低计算复杂度（比如reformer、informer等），而是在硬件上，它重新制定了Attention计算在GPU上不同计算单元间的调度，以此来达到真实场景下的提速。GPU硬件上本人不是太行，对细节感兴趣的可以看这篇blog。实际运用中，FlashAttention可以对推理提一些速（10%-40%），但是效果感觉并不是那么惊人。 RoPE embeddings。见Llama章节介绍。 Multi-Query Attention。见Llama章节介绍。 Parallel Attetnion and Feed-Forward layers。\b传统串行的attention+feed-forward不同，Falcon采用了并行的方式（我记得最初好像是由PaLM提出来的），相当于对于input，分别进行self-attention和feed-forward，然后再加起来（同时还需要加上原input，因为ResNet）。大概如下图： 5.3. ChatGLM ChatGLM采用的是GLM架构，是一个挺有意思的架构。它的诞生其实是在LLM井喷之前，在预训练模型被Bert、GPT和T5三分天下的时候。GLM也是采用的decoder-only transformer，但是它与GPT很大的不同在于，它并不只是做NLG（next token prediction），同样的他还要做NLU（Bert的强项）。GLM巧妙的设计了一个机制，autoregressive blank filling，使得它在预训练的时候可以既做理解训练又做生成训练。机制如下（可以对照下面的官方案例图一起看）： 对于收集到的某一条语料句子X={x1,x2,x3,x4,x5,x6}X=\\{x_1,x_2,x_3,x_4,x_5,x_6\\}X={x1​,x2​,x3​,x4​,x5​,x6​}，首先从里面采样子句子（连续token），比如图中的sub-sentence1：{x3}\\{x_3\\}{x3​}和sub-sentence2：{x5,x6}\\{x_5,x_6\\}{x5​,x6​}。 将原始句子分为两部分: PartA：挖掉采样子句子后剩下的原句子，并将挖掉的部分留空白用[M]表示。这儿即{x1,x2,[M],x4,[M]}\\{x_1,x_2,[\\mathtt{M}],x_4,[\\mathtt{M}]\\}{x1​,x2​,[M],x4​,[M]}。 PartB：刚才采样的子句子，这儿即{x3,x5,x6}\\{x_3,x_5,x_6\\}{x3​,x5​,x6​}。 重排序列组合生成Input。先将PartB里的子句子shuffle随机排序，比如这儿sub-sentence2就变到了sub-sentence1前面，变成了{x5,x6,x3}\\{x_5,x_6,x_3\\}{x5​,x6​,x3​}。然后加上开始token（[S]）。最后将PartA和当前PartB拼接起来。Input就是{x1,x2,[M],x4,[M],[S],x5,x6,[S],x3}\\{x_1,x_2,[\\mathtt{M}],x_4,[\\mathtt{M}],[\\mathtt{S}],x_5,x_6,[\\mathtt{S}],x_3\\}{x1​,x2​,[M],x4​,[M],[S],x5​,x6​,[S],x3​}。 给重排序后的序列进行位置编码，使用了2D-positional encoding。第一个位置编码是原始句子中的位置，填空的部分（[M]）用一个位置标记。第二个位置编码是对于PartB的子序列标记其原有的内在顺序。 将上述Input和位置编码放入GLM，进行双目标训练：完形填空（靠PartA）和下文生成（靠PartB）。所以其Attention掩码的设计就不是经典的causal mask，而是prefix mask。PartA是非causal的，相互之间可以看到，进行上下文理解，做完形填空。而PartB里就是causal的，做下文预测。从这儿可以感受一下，PartA就类似Bert，同时PartB的shuffle设计还将不同[M]之间变得非独立（克服了Bert里独立性的假设）。 除了以上最核心设计的区别，GLM在在原始的transformer架构上还： 更改了LayerNorm和ResNet的顺序。改为先LayerNorm后Add。 用单个线性层来进行最终token的预测 激活函数用GeLUs替代ReLU 5.4. Qwen 讲道理，在我已有的实践中，千问是用得比较多的，所以po一下它的家族系列： 目前开源社区里的主要是蓝色和紫色的部分，RLHF部分应该是千问商用API的😄。 看了千问的技术报告，架构上基本同Llama。一些小调整在于： embedding层和output层的权重矩阵分离（传统transformer应该是tied weights，就是这两个是共享的。Llama2里好像没有说这个） Input到QKV的转换加入了bias（很多是没有加的），即Q=I×Wq+bqQ=I \\times W_q+b_qQ=I×Wq​+bq​ 架构之外，在推理上，关于长序列推理加入几个重要的training-free技术： dynamic NTK。具体可见这篇知乎文章 LogN-Scaling。基于熵不变性为出发点的外推技术，推导可以看这篇blog window attention。类似于Sparse Attention，不过这儿只是在推理的时候才用到，训练的时候不用。将attention的计算只限定在一个固定窗口长度中。 5.5. Mixtral Mixtral的出名是提出了一种新的行之有效（评估结果很好）的架构模组：MoE（Mixture of Experts）。我们先介绍原始的Mixtral-7B，然后再引出Mixtral-MoE（8✖7B）。 Mixtral-7B在架构上基本还是采用的Llama架构，在此基础上最核心的改变是使用sliding window attention（SWA，道理同sparse attention）: 。 由于decoder blog一般都会重复很多次，所以虽然每次当前token的注意力都只在只限定在一个窗口里，但是一层一层传递的时候，这个token会受到较多前文的影响（指数增加，如上图最右边例子）。理论上说是这么一说，原文中window size设置的也只是最长回望上下文长度的一半😄。 推理性能上也做了两个调整，适配SWA：Rolling Buffer Cache和Pre-fill Chunking。都是在cache上的调整，具体可以见原文。 Mistral-7B架构上感觉很普通，但是也有不错的评估效果，我认为应该是在数据上做了不少文章。 在Mistral-7B基础上，该团队提出了MoE的模型（Mistral 8*7B），获得了较大的成功。Mistral-MoE提出了一种新的transformer decoder-only架构变种。将decoder从GPT开始一直沿用的[ Attention + Add&amp;Norm -&gt; FeedForward + Add&amp;Norm ]中的FeedForward layer替换成了MoE layer，借鉴一下下图： 。 MoE首次提出是在20年（这儿），之前没什么名气。这个layer里主要包含两个东西：router和expert。MoE整体实现是个加权求和，router给出了加权的权重，expert给出求和的每个元素。所以，对于给定的Input xxx，有nnn（这是个超参数）个experts的MoE层的output可以表示为： ∑i=1nG(x)i⋅Ei(x)\\sum_{i=1}^n G(x)_i \\cdot E_i(x) i=1∑n​G(x)i​⋅Ei​(x) 这儿，G(x)iG(x)_iG(x)i​表示gating network（即router）的nnn维output中的第iii个（对应第iii个expert的权重），Ei(x)E_i(x)Ei​(x)是第iii个expert network的output。 所谓的gating network G(x)G(x)G(x)是用softmax计算得到的权重，但是这儿的softmax是取的top K（K为超参数）个元素的softmax： G(x):=Softmax(TopK(x⋅Wg))G(x):=\\mathrm{Softmax}(\\mathrm{TopK}(x\\cdot W_g)) G(x):=Softmax(TopK(x⋅Wg​)) 相当于x先进行线性转换，在得到Rn\\mathbb{R}^nRn的向量里只取最大的前K个来进行softmax，只有前K个有权重（概率），后n-K个权重都为0。比如原文中是8个experts，每次只取前2个来进行softmax。可以想象成每一个入参都由8个专家审核，但是每次只会采纳2个专家的建议。 “专家的建议”E(x)E(x)E(x)则由SwiGLU激活单元表示（没有过多的解释，很好奇为什么不是SwiGLU的FFN🤔️）。 MoE最核心的就是以上了。整体感觉MoE没有什么复杂的操作，但是却实现了一个比较好的效果，就很“漂亮”。然后虽然名字叫Mixtral-8*7B，但是实际参数量并不是56B，只有47B，因为8个专家只有在expert network上各自独立参数，其他的在attention层、resnet、norm、router、output层都是共享的相同权重矩阵。 ","link":"https://lostlau.github.io/post/attention_to_gpt/"},{"title":"数值计算的精度与量化技术","content":"为了节约大模型的部署资源，最有效的方法就是“量化（quantization）”，这儿的量化可不是量化投资（开玩笑，🐶）里的量化，是指降低模型参数的精度，即将原本的高精度浮点数（如32位或16位浮点数）转换为更低位数的整数或更低位的浮点数（如8位整数或浮点数），从而减少存储需求和计算时间的方法。想要弄懂量化首先需要理解数据的精度。 从FP32开始 计算机处理数字类型包括整数类型和浮点类型，IEEE-754标准定义了浮点类型数据的存储结构。一个浮点数由三部分组成，以最常见的FP32(Float Point 32)为例： Sign：最高位用1位表示符号位，1表示负数，0表示正数，记为S\\mathrm{S}S Exponent：中间8位表示指数位，记为E\\mathrm{E}E Mantissa：低位23位表示小数位，记为M\\mathrm{M}M 【举个例子】 我们拿12.75举例，来看十进制和FP32二进制之间的转换。 首先，分为整数位和小数位，整数位是12，转换为二进制是110011001100（因为12=23+2212=2^3+2^212=23+22，换底之后=103+10210^3+10^2103+102）。然后是小数位0.75=2−1+2−22^{-1}+2^{-2}2−1+2−2，转化为二进制后就是0.110.110.11。小数部分的转换也可以用乘2\b取整数部分，如果整数为1那么该指数位记一次，剩余的小数部分再继续进行判断，一直迭代循环直到小数部分为0。把整数和小数的二进制加起来就是总的二进制转换结果：1100.111100.111100.11，再转换成科学计数指数形式：1.10011×231.10011\\times2^31.10011×23。这儿的指数级数3就是原始指数位，不过再根据IEEE-754的规范，FP32的指数部分要加127偏移，调整为3+127=130，然后再转为二进制，最终指数位为10000010。小数部分10011补齐为23位后得到最终小数位，符号位为0，三个部分拼起来就是 0 10000010 100110000000000000000000\\ 10000010\\ 10011000000000000000000 0 10000010 10011000000000000000000 使用转换工具验证，结果一致。工具的上面把三个部分都表示出来了，复选框打勾表示对应位置二进制数为1，否则为0。 将FP32复原为十进制，那么把上面的转换逆转过来，可以有公式： x=(−1)S×2g(E)−127×g(1.M)x=(-1)^{\\mathrm{S}} \\times 2^{g(\\mathrm{E})-127} \\times g(1.\\mathrm{M}) x=(−1)S×2g(E)−127×g(1.M) 其中ggg表示二进制转十进制。 那么回到12.75的例子，\b10000010转十进制就是130，1.10011转十进制为1+2−1+2−4+2−5=51321+2^{-1}+2^{-4}+2^{-5}=\\frac{51}{32}1+2−1+2−4+2−5=3251​。\b所以最终x=2130−127×5132=514=12.75x=2^{130-127}\\times \\frac{51}{32}=\\frac{51}{4}=12.75x=2130−127×3251​=451​=12.75。 FP32搞清楚，FP16、FP64类似，只是指数位和小数位的长度不一样： 类型 符号位长度 指数位长度 小数位长度 偏移 半精度\bFP16 1 5 10 15 \b单精度FP32 1 8 23 127 \b双精度FP64 1 11 52 1023 所以12.75的FP16的表示为： 0 10010 10011000000\\ 10010\\ 1001100000 0 10010 1001100000 FP64的表示为： 0 10000000010 10011000000000000000000000000000000000000000000000000\\ 10000000010\\ 1001100000000000000000000000000000000000000000000000 0 10000000010 1001100000000000000000000000000000000000000000000000 不同精度的优劣 对比FP32和FP64，其明显优势是： 节约储存空间。由于FP32只占32位，是FP64的一半，所以也只会占用一半的存储空间。在相同的GPU下，能装的模型参数的就变多了，或者相同模型参数训练/推理时的batch_size可以更大。 提高计算速度。同样的两个数计算，位数少了，计算量也就相应降低了，所以算得更快。 那么，其劣势也很明显： 精度问题。小数位数少了，能表达的精度（小数点后的位数）就降低了。比如同样是表示1/3，FP16、32、64转换出的精度分别如下： 精度越高，数据表示和计算越准确，模型训练拟合出来的参数更精准，这个需要看我们对具体模型精度的要求。 溢出问题。指数位少了，那么能表示的整数范围也变小了，有些很大的整数可能就无法表示了。能表示的范围见下图，比如FP16就只能表示[-65504,65504]之间的数。 混合精度 既然FP32和FP16长短各有优缺点，那我们就可以采取混合使用的方法，在模型训练的不同步骤使用不同的精度： 把神经网络权重参数由初始化的FP32转为FP16； 用FP16进行前向和后向计算，并进行梯度计算； 把FP16的梯度转为FP32； 使用FP32的梯度和学习率learning rate相乘； 使用FP32更新网络权重，得到FP32的更新后的权重。 以上步骤不断循环进行。简单来讲就是使用梯度更新权重的时候用FP32，因为梯度乘上学习率后一般数值都比较小，使用FP32能防止精度不够。 混合使用精度的时候，有一个&quot;损失缩放&quot;的技术，在反向计算前，将得到的损失扩大一个倍数，避免数据太小精度不够变为0了，扩大后在FP16可表达的范围内，反向计算后，再把梯度缩小同样的倍数，确保最后数值是对的。 混合精度可以让模型训练的速度更快，资源占用更低，同时保持一个不错的准确度。 BF16、TF32 FP16的指数位只有5位，小数位10位，能表示的整数范围有点小，于是谷歌为了深度学习特别是他们的TPU定义了一种新的格式Brain Floating Point 16，简称BF16。和FP16比，总长度都是16位，只是把指数由5位变为了8位（和FP32一样，能有其相同的整数范围），小数位数缩短到了7位。 英伟达根据其GPU的需要定义了TF32(Tensor Float 32)，指数位8位（和FP32、BF16一样），小数位10位（和FP16一样，比BF16长），其实就是比BF16多了3个小数位。虽然叫TF32，但是并不是32位的，有点‘高效’32位的意思，范围搞到FP32，同时精度保持FP16。 INT8量化 上面介绍完浮点数的类型和精度，我们可以知道使用FP16可以比FP32消耗的显存缩减一半，同样的我们可以进一步降低精度，使用比如FP8来继续减半显存的消耗，不过这样带来的模型输出准确性的降低可能是灾难性的。为了缓解这种准确性的降低同时可以进一步降低资源消耗，引入了所谓的量化技术，我们来看一下Int8的量化。 量化的核心思想就是放缩+rounding，将原始的数值通过放缩+rounding转换成一个范围内的整型数值（INT）来表示原数值。INT8就是转换到[-128,127]范围内。公式表示： xq=Clip(Round(xf∗scale))x_q = \\mathrm{Clip}(\\mathrm{Round}(x_f*\\mathrm{scale})) xq​=Clip(Round(xf​∗scale)) 其中 Round 表示四舍五入都整数，Clip 表示将离群值(Outlier) 截断到 [-128, 127] 范围内。对于 scale 值，通常按如下方式计算得到： scale=127/max⁡(∣xf∣)\\mathrm{scale}=127/\\max(|x_f|) scale=127/max(∣xf​∣) 反量化的过程为： xf′=xq/scalex_f^\\prime = x_q/\\mathrm{scale} xf′​=xq​/scale 以下是一个INT8量化与反量化的例子： 当进行矩阵乘法时，可以通过组合各种技巧，例如逐行或逐向量量化，来获取更精确的结果。举个例子，对矩阵乘法，我们不会直接使用常规量化方式，即用整个张量的最大绝对值对张量进行归一化，而会转而使用向量量化方法，找到 A 的每一行和 B 的每一列的最大绝对值，然后逐行或逐列归一化 A 和 B 。最后将 A 与 B 相乘得到 C。最后，我们再计算与 A 和 B 的最大绝对值向量的外积，并将此与 C 求哈达玛积来反量化回 FP16。 同时，上述的量化方式如果遇到离群点将会导致scale变得很小，那么大部分正常点转换后就会集中在一坨或者一个点上，反量化后会有非常大的误差。那么一种方法是在矩阵运算的时候，先根据异常值进行分解，包含异常值的那部分矩阵进行正常的FP16/FP32的运算，无异常值的部分进行量化运算，之后再把结果聚合起来。具体的一些介绍可以看这篇blog。 INT8量化后的模型大小会变成FP16的一半，其真实计算时的参数数据又可以反量化到FP16/32，保持了计算精度。在此基础上，想进一步再降低资源消耗，那么可以进行INT4的量化，道理和INT8一样，只不过放缩的表示区间缩到了[-8,7]。 ","link":"https://lostlau.github.io/post/quantization/"},{"title":"Docker 指令记录","content":" 常用参数 docker build -f：指定要使用的dockerfile的文件路径 -t/--tag：镜像的名字及标签，比如 x_webapp:v1 docker run -d：让容器在后台运行，并返回容器ID；没这个run的时候会进入容器内 -i：以交互模式运行容器，通常与 -t 同时使用 -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用 --name：为容器命名，比如 --name python_flask表示将该容器命名为python_flask -p：端口映射（宿主机端口:容器端口），\b-p 8080:22 表示把容器内的22端口映射到宿主机上的8080端口 --gpus：将宿主机上的GPU资源加载到容器中，--gpus all 表示加载所有可用GPU。 --privileged：给容器权限去连接主机上的其他设备，就像使用这些设备或资源的主机一样，比如连接到nvidia的GPU资源。 -m：设置容器能用的内存最大值 -v：挂载一个路径地址，比如 -v /root/xxx/docker_tuto : /workspace 表示将宿主机上/root/xxx/docker_tuto/文件夹地址挂载（视为镜像）到容器内的/workspace上 常用场景案例 手动建立镜像 docker build -f Dockerfile -t python_flask:v1 . 容器化并运行 docker run -d -p 8090:8090 --name flask_app python_flask:v1 容器化并运行（同步容器内时间与宿主机系统时间一致） docker run -d -p 8090:8090 -v /etc/localtime:/etc/localtime --name flask_app python_flask:v1 容器化并运行（使用gpu资源，挂载文件路径，使用远程hub的pytorch/pytorch镜像） docker run -p 10001:22 --gpus all -itd --name pytorch_gpu --privileged=true -v /root/xxx/docker_tuto:/workspace pytorch/pytorch /bin/bash 查看正在使用的容器 docker ps 查看所有容器 docker ps -a 查看所有镜像 docker images 查看容器logs(下述xxx用containerID前三位替代) docker logs xxx 交互模式去容器中操作命令行(下述xxx用containerID前三位替代) docker exec -it xxx bash 停止容器(下述xxx用containerID前三位替代) docker stop xxx 删除容器(下述xxx用containerID前三位替代) docker rm xxx 删除镜像(下述xxx用imageID前三位替代) docker rmi xxx 额外参考 docker官方指令参考 docker-从入门到实践：介绍了docker的基本概念和原理，以及其相关的生态（比如kubernetes）的介绍 ","link":"https://lostlau.github.io/post/docker_cli/"},{"title":"PMP学习&考试 - 总结","content":"该系列并没有更新起来就结束了，因为说实话有点无聊😂。 因为疫情考试延期+取消，陆续搞了快一年，最后在23年3月考了，5月出成绩，通过了。 \b一些感悟和经验分享 首先需要报班，pmi要求了一定学习时常。排除这个硬性规定，报班还是要学得快一些，因为他们有总结好的资料，比看官方的PMbook还是要快很多。 pmp的知识框架其实挺奇怪的，和我们平时的知识构建框架有些不同，最开始入场的时候可能有一些奇怪，特别是对于理科思维的，始终感觉里面有一些逻辑就很不严谨。 PMBook主要讲的（至少6版，之后的版本不一定）都是传统的项目管理（瀑布式的），系统学了还是对整个流程的认知很有帮助。然后考试目前不倾向于考waterfall了，主要还是偏敏捷。其实我个人感觉敏捷要简单一些（至少考的这部分敏捷）。 针对考试的话，想效率高，可以倍速看网课+自己看讲义复习+刷题（大的培训班都有题库资源），每天弄4个小时的话，这样差不多1个月应该就比较稳了。时间分配1：2：2吧，听网课感觉主要是听一遍框架，里面的细节知识还是自己看得快一点，刷题也是学知识的一个途径，遇到不会做的再回去看值个知识点，把它记下来，这样感觉效率好一点。 虽然我不是专职做项目管理的，但是平时工作中“项目”是到处都存在的，都有介入“管理”的机会，系统学了之后还是对做项目的时候的观察到的管理方式有一些自己的看法，然后同时可以设身处地的想如果我是项目经理可以在什么场景怎么做会更好，或者是否该场景下这种方式就是最优的了。anyway，工作中接触的人通常都既是伙伴也是对手的，知己知彼能够把问题看得更透。 ","link":"https://lostlau.github.io/post/pmp_conclu/"},{"title":"Prompting Engineering","content":"吴恩达+OpenAI的教程总结（准则主要作用于ChatGPT，其他模型不一定适用） 一、Prompting的准则 准则1. 写一个清晰的和特别的指令 要点1. 使用特殊符号 &quot;&quot;&quot; ``` --- &lt;&gt; &lt;tag&gt;&lt;/tag&gt; 要点2.要求结构化的输出 HTML，JSON 要点3.检查条件是否被满足 check assumptions required to the task 要点4.少样本prompting 准则2. 给模型思考的时间 要点1. 明确完成任务所需的步骤 step1、step2、step3、...stepN 要点2. 指示模型去得到它自己的解答而不是急于给出一个结论 某些场景可以让LLM先自己给出方案/解答，然后再让它对比已有的（输出的）方案/解答，它会给出更好的比较。 模型局限性 需要考虑到模型的局限性，模型可能会给出似是而非但是实际上并不正确的回答（这种现象被称为'hallucinations'）。降低hallucinations的方法： 找到相关信息 根据相信信息回答问题 二、迭代式的提示工程开发 合适的提示词/框架需要通过迭代的方法去实现，就像传统ML的训练调参过程一样： 初始化，随便尝试某个指令 分析得到的结果和你内心想要的结果的差异 多花时间时间思考，更新指令，使其更清晰明了 使用一系列例子来细化提示 ","link":"https://lostlau.github.io/post/prompting-engineering/"},{"title":"transformer-based时序领域算法总结","content":"由于工作原因，近段时间对时序领域的机器学习任务（forecasting、regression、classification、clustering、segmentation、anomaly detection等）有较多接触，同时我也对这一领域有较大的兴趣，于是开个帖子作为记录。本篇主要记录基于transformer的神经网络算法，它们适用的大多数任务场景为时序的forecasting。 本篇将会持续更新。 Vanilla Transformer 原文：Attention is All You Need 经典的出发点，Q,K,VQ,K,VQ,K,V，原始点积attention出名的地方。它是针对机器翻译问题而提出的网络结构。 这里就不过多的介绍了，网上好的解读非常多，比如这儿。 Sparse Transformer 原文：Generating Long Sequences with Sparse Transformers 改造中间的attention层，提升计算效率，O(L2)\\Omicron(L^2)O(L2)降低到了O(LL)\\Omicron(L\\sqrt{L})O(LL​)。有两种模式：strided和fixed。 strided是只关注当前位置的前L\\sqrt{L}L​个，相当于移动窗口；fixed是固定窗口，每第L\\sqrt{L}L​个中的某个位置只关注当前L\\sqrt{L}L​个中且不超过当前位置（在该位置左边）的这些位置。 Reformer 原文：Reformer: The Efficient Transformer 属于原始transformer的改造。在原始transformer的基础上，提升了计算效率，主要是以下两点： 基于局部敏感性哈希算法(Locality Sensitive Hashing, LSH)的attention机制替代经典点积(dot-production) attention机制，将计算复杂度从O(L2)\\Omicron(L^2)O(L2)降低到了O(L)\\Omicron(L)O(L)。从其实验结果来看，任务预测效果与原始attention相差不大。 使用可逆(Reversible)残差连接来代替传统残差连接，好处是在前向过程中前N−1N-1N−1层的中间值和输出都不需要存储了，只保留最后一层的输出，极大降低了内存消耗。 Informer 原文：Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting 针对长时时序预测LSTF(long sequence time-series forecasting)，对transformer进行改造。作者认为长时时序预测模型需要两方面的能力： 卓越的长时序列的特征表示能力 高效的针对超长序列出入参的操作 作者认为transformer在1方面有不错的能力，所以主要针对第2个方面（即计算效率）对transformer进行改进。改进包括以下3个点： 提出ProbSparse self-attention机制来替代原始self-attention，时间复杂度和内存消耗都降为O(LlogL)\\Omicron(L\\text{log}L)O(LlogL)。用KL-divergence去度量每个softmax(qKT/d)\\text{softmax}(qK^T/\\sqrt{d})softmax(qKT/d​)与均匀分布(1L,…,1L\\frac{1}{L},\\dots,\\frac{1}{L}L1​,…,L1​)的差异，只把差异最大的前logL\\text{log}LlogL个queries挑出来，其他queries置0，再用转换后的稀疏query矩阵Qˉ\\bar{Q}Qˉ​来计算self-attention（还是原始点积计算）。 提出self-attention蒸馏操作来优先考虑占主导地位的attention分数，用于layers stacking中，极大降低内存消耗。\b使用convolution layer和maxpooling连接各个self-attention，降低隐层特征表示的维度。 提出生成式decoder来获得长序列输出。通过一次前向计算，进行多步预测来预测长序列的所有输出，decoder的输入也相应的需要调整。 原文写得不是太好读，细节的解读推荐参考这篇。 Autoformer 原文：Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting 仍然是解决长时时序预测问题，针对经典transformer的缺陷： 难以捕捉复杂的时序点上的内在模式和依赖关系 point-wise的attention的计算复杂度阻碍了长序列特征表示的学习 提出了autoformer框架，核心是把原始序列进行分解（按经典时序分解：trend+seasonality），然后rolling变成很多子序列，从而把特征表示学习中的信息整合过程的颗粒度放到series-level（而非point-wise）。特征表示的学习使用auto-correletion机制来实现。所以，核心创新是以下两点： 序列的分解模块。分解不在原始序列上直接分解，而是把分解放到神经网络里，作为一个block，动态的学习其分解模式。该模块中，通过移动平均获得趋势项，然后原序列减去趋势项得到季节项。季节项是核心要关注学习表示的项，趋势项在decoder里最终使用线性自回归来学习weights。 auto-correlation机制。受随机过程启发，作者定义原序列和滞后序列的相关性系数。选出top K个相关性最好的滞后序列的滞后时间，然后借助softmax将这K个相关性score转换成概率，把这些概率再和其对应的原滞后序列进行加权求和，完成信息聚合。滞后相关性系数的计算作者根据维纳-辛钦定理借助FFT来计算，降低了计算复杂度。 整体框架结构如下图： 具体细节的解读，推荐这篇。 Pyraformer 原文：Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series modeling and forecasting 设计的初衷并不只局限于长时时序预测，同样对短时时序预测和分类、非监督任务也适用。其改造的核心目的也是在增强计算效率的同时使得时序间的依赖关系能够被更好的表示（反映在预测效果上就是更好）。 整体框架结构如下图： 三个核心改造要点： PAM (pyramidal attention module)。将原始\b\b时间序列按照给定的步长进行聚合（聚合方式参照后续CSCM模块），将时间跨度更长的信息提炼出来，比如原始是按小时的信息，向上提炼可得到按日的信息，按月的信息，按年的信息。直观的可见下图，最下层就是原始时序，越往上就是时间跨度越大的聚合信息，此处是设定的每两个时间跨度一聚合。层与层之间称为inter-scale，同一层node与node之间称为intra-scale，抽象了不同时间跨度的信息聚合，所以作者称其是multi-resolution（多分辨率）。在此结构基础上提出了attention的新型表达式，只关注与当前节点相邻的节点，即inter-scale下的父节点和子节点，intra-scale中的临近节点（多少步以内算临近是预设参数）。经典点积attention计算中只考虑有上述“相邻关系”的Q−KQ-KQ−K pair的计算，使得其计算复杂度极大降低（O(L)\\Omicron(L)O(L)）。 CSCM (coarser-scale construction module)。这一模块就是去构建金字塔结构，信息的聚合通过卷积实现。最后将金字塔每一层的nodes最后concat起来，再做一个线性转换得到输出。 预测模块。提出两种策略，一种是直接多步预测，类似于Informer，直接一步预测待预测的所有未来序列点（将PAM中每一层的最后的nodes提取concat起来再过一个全链接层）；第二种是类似经典transformer，采用decoder思想，加入两层attention+mask来进行进行。decoder入参进行mask操作（未来信息点填0），然后PAM。 FEDformer 原文：FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting 第一出发点终于不是speed-up Transformer了😅，文章把改进Transformer的表示能力作为了第一动机，认为原始的attention无法很好的捕捉全局层面的特征，于是提出了两个思路来优化： trend+seasonality分解，但是是基于Kologrov-Smirnov distribution test的思路。 结合傅立叶变换，将频域的信息融入进attention，强化对时序内在依赖的捕捉。 在以上改进的方法上，同时发现计算复杂度也得到了降低，最终实现预测效果和计算效率的同步加强（总感觉很强行😄）。整个网络结构如下，和autoformer基本差不多，除了核心的两个block：attention block和decomp block改为新的方式（即frequency enhanced block和MOE decomp）。 FEB(frequency enhanced block)。见下图，先过一个linear层，接下来做傅立叶变换(F\\mathcal{F}F)。然后敲黑板，此处sampling就是随机采样M个（预设的）分解后的信号模式（计算复杂度降低的关键）。DDD表示多变量时序场景对应的时序特征数量。蓝色块的RRR是可学习的参数weight，这一步线性转换目的是考虑特征间的相关性，将interaction纳入到特征表示。之后填0再逆向傅立叶变换，得到最终输出。 FEA(frequency enhanced attention)。见下图，流程同经典的cross-attention，只不过是在频域的随机采样上进行的。 MOE decomp。采用了一些随机时间窗口去做AvgPooling，然后将这些产生的结果加权得到最终trend，加权平均的权重是可以学习的参数。 以上是基于傅立叶变换分解的attention block，文章也提出了基于小波变换分解的attention block，具体细节有所不同（时频域同时考虑），但是high-level的思想是一样的，就不展开了。 ETSformer 原文：ETSformer: Exponential Smoothing Transformers for Time-series Forecasting 借鉴Autoformer的思路（同样也有和FEDformer相近的思想），将原始时序做分解，然后在关注的序列上进行attention处理。目的也是为了更高效的捕捉时序间依赖性，提告预测精度。其核心创新主要是以下两点： 时序分解参照了Holt-Winter's加性方法，将时序分解成了三个成分：level+growth+seasonality 将移动平滑的思想融入到了针对growth项的提炼里（ES Attention），其背后的intuition是离当前时间点越近的前序时间点应该有更大的注意力权重，注意力权重应随时间点的前移而逐渐衰退。 整体如下图，左边是encoder，将原始时序先通过卷积层进行embedding，然后通过FA提取seasonality，将除seasonality的部分通过(MH-)ESA继续进行growth的提取，剩下的部分通过Holt-Winter's里的方法（还是指数平滑的思想）生成level。右边是decoder，这儿没有原始输入，而是用encoder分解得到的seasonality和growth作为输入，来生成（重新组合成）未来的预测。图中展示了N个stack叠加的情况，即分解N层再重新聚合N层的场景。 FA和ESA的逻辑： Feature Attention(FA)。这个block主要就是根据傅立叶变换寻找top K个最强的子信号然后重组构成seasonality。 ES Attention。我的理解是这个block将原始的attention中的attention weight矩阵表示成了基于衰退系数α\\alphaα和时间滞后步数的一个指数移动平均系数矩阵（见原文附录A.1）。 Non-stationary Transformer 原文：Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting 这篇作者的出发点更多的是借鉴了传统统计时序分析的视角，以平稳性问题为切入口，将针对非平稳性的处理纳入到传统transformer里。该框架在面对非平稳序列时有两个显著的优势： 能够通过平稳化方法之后针对平稳序列（i.e.在不同的时间窗口间子序列都有相近的统计性质）学习得到良好的预测效果 有效counter了过平稳化问题（由于平稳化处理导致的attention提取的时序依赖关系不显著问题） 为了达到以上效果，该网络的设计框架可见下图，主要有以下两个核心要点： 平稳化过程的设计。首先通过归一化将原始时序input都scale到N(0,1)\\mathcal{N}(0,1)N(0,1)，然后才是通常的transformer操作（embedding+encoder+decoder等），最后还有一个反归一化处理，即将开始归一化得到均值方差反作用在transformer的出参上，之后才是真正的output。 De-stationary Attention。这个block替代原始的attention，目的是在平稳化后的序列上进行attention操作时能够加入原始非平稳序列时序间蕴含的相互依赖，从而减缓过平稳化问题。设计得比较巧妙，作者首先非常宽松的证明了归一化序列的attention distribution和原始序列的attention distribution（i.e. attention之后再softmax生成的那个权重）之间的关系，建立等价关系还需引入两个因子（de-stationary factors），即有了这两个因子后通过归一化序列的attention权重可以近似表达出原序列的attention权重。那么如果得到这两个因子具体的值呢？作者提出通过一个单独的MPL-block去学习它们（图中Projector）。 TimesNet To Be Continued.. ","link":"https://lostlau.github.io/post/transformer_based_ts/"},{"title":"PySpark学习（三） - DataFrame和SQL","content":"pyspark学习的第三篇会介绍DataFrame和其相关的操作运用。 \b引言 PySpark DataFrame是惰性计算的，可以算是RDD的上层类。同时，DataFrame也可以看成是行DataSet。 以下将会记录DataFrame的创造、操作和UDF的运用，最后记录了SQL查询作用于DataFrame。 from pyspark.sql import SparkSession from pyspark import SparkFiles, Row from pyspark.sql.types import * from pyspark.sql.functions import pandas_udf,PandasUDFType import pandas as pd import numpy as np import datetime spark = SparkSession.builder.appName(&quot;example_sparksql&quot;).config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;).getOrCreate() spark.sparkContext SparkContext Spark UI Version v3.2.1 Master local[*] AppName example_sparksql 创造Spark DataFrame 以下展示常用的4种方式来创建Spark DataFrame schema = StructType([ StructField(&quot;age&quot;, IntegerType(), True), StructField(&quot;name&quot;, StringType(), True) ]) ## 1.read from https print(&quot;---- 1.read from https and schema given ---- \\n&quot;) url = &quot;https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/people.json&quot; spark.sparkContext.addFile(url) df1 = spark.read.json(SparkFiles.get(&quot;people.json&quot;),schema=schema) print(&quot;df1's content is: &quot;) df1.show() print(&quot;df1's schema is: &quot;) df1.printSchema() ## 2.create from pd.DataFrame print(&quot;---- 2.1. duplicate a same table in pd.DF then create from it ---- \\n&quot;) df2 = spark.createDataFrame(pd.DataFrame({&quot;age&quot;:[None,30,19],&quot;name&quot;:[&quot;Michael&quot;,&quot;Andy&quot;,&quot;Justin&quot;]})) print(&quot;df2's content is: &quot;) df2.show() print(&quot;df2's schema is: &quot;) df2.printSchema() print(&quot;---- 2.2. find the diff with 1. and adjusting ---- \\n&quot;) print(&quot;change float nan to Null, then check df2's content:&quot;) df2 = df2.replace(float('nan'),None) df2.show() print(&quot;adjust df2's schema:&quot;) df2 = df2.withColumn(&quot;age&quot;,df2.age.cast(IntegerType())) df2.printSchema() ## 3.create by Row print(&quot;---- 3. Using [Row(.)] to create DataFrame ---- \\n&quot;) df3 = spark.createDataFrame([ Row(age=None, name='Michael'), Row(age=30, name='Andy'), Row(age=19, name='Justin') ], schema=&quot;age int, name string&quot;) print(&quot;show df3's schema:&quot;) df3.printSchema() ## 4.create from rdd print(&quot;---- 4. Creating RDD then convert to DataFrame ---- \\n&quot;) rdd = spark.sparkContext.parallelize([ (None, 'Michael'), (30, 'Andy'), (19, 'Justin') ]) df4 = spark.createDataFrame(rdd, schema=schema) print(&quot;show df4's schema:&quot;) df4.printSchema() ---- 1.read from https and schema given ---- df1's content is: +----+-------+ | age| name| +----+-------+ |null|Michael| | 30| Andy| | 19| Justin| +----+-------+ df1's schema is: root |-- age: integer (nullable = true) |-- name: string (nullable = true) ---- 2.1. duplicate a same table in pd.DF then create from it ---- df2's content is: +----+-------+ | age| name| +----+-------+ | NaN|Michael| |30.0| Andy| |19.0| Justin| +----+-------+ df2's schema is: root |-- age: double (nullable = true) |-- name: string (nullable = true) ---- 2.2. find the diff with 1. and adjusting ---- change float nan to Null, then check df2's content: +----+-------+ | age| name| +----+-------+ |null|Michael| |30.0| Andy| |19.0| Justin| +----+-------+ adjust df2's schema: root |-- age: integer (nullable = true) |-- name: string (nullable = true) ---- 3. Using [Row(.)] to create DataFrame ---- show df3's schema: root |-- age: integer (nullable = true) |-- name: string (nullable = true) ---- 4. Creating RDD then convert to DataFrame ---- show df4's schema: root |-- age: integer (nullable = true) |-- name: string (nullable = true) 操作Spark DataFrame df = df4 查看数据 df是惰性求值的，通过.show()来查看数据。df.columns可以展示表头。 如果需要开启及时求值(‘即查即得’)模式，需要去配置项里进行更改：将spark.sql.repl.eagerEval.enabled设置为True。同时spark.sql.repl.eagerEval.maxNumRows可以设置最大行数。 df.collect()其实在df内含的rdd上做collect()操作，也可以展示df的信息。取前/后几排操作可以使用df.take()和df.tail()实现，不同这些也都是在rdd上进行的操作，所以结果的类型并不是一个dataframe df同样可以转化为pandas.DataFrame。 df.show() +----+-------+ | age| name| +----+-------+ |null|Michael| | 30| Andy| | 19| Justin| +----+-------+ df.columns ['age', 'name'] spark.conf.set('spark.sql.repl.eagerEval.enabled', True) df agename nullMichael 30Andy 19Justin df.collect() [Row(age=None, name='Michael'), Row(age=30, name='Andy'), Row(age=19, name='Justin')] df.take(1) [Row(age=None, name='Michael')] df.toPandas() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age name 0 NaN Michael 1 30.0 Andy 2 19.0 Justin 筛选数据 pyspark里这一块最核心的类型是Column，所有的Column都可以通过.select()方法来获取展示。 spark.conf.set('spark.sql.repl.eagerEval.enabled', False) df.select(&quot;age&quot;).show() +----+ | age| +----+ |null| | 30| | 19| +----+ df.filter(df.age&gt;20).show() df[df.age&gt;20].show() +---+----+ |age|name| +---+----+ | 30|Andy| +---+----+ +---+----+ |age|name| +---+----+ | 30|Andy| +---+----+ df.withColumn(&quot;agenull_flag&quot;,df.age.isNotNull()).show() +----+-------+------------+ | age| name|agenull_flag| +----+-------+------------+ |null|Michael| false| | 30| Andy| true| | 19| Justin| true| +----+-------+------------+ apply自定义函数 使用pandas UDF（User Defined Function）可以很简单的自定义想要应用的函数。它的作用机理是：由Spark执行，使用Arrow（一种用于内存计算的列式数据结构的库，可以极大的增强JVM和Python之间的数据传输效率，具体可见这篇文章）传输数据，通过Pandas执行数据计算，允许向量化操作。 Pandas UDF通过使用pandas_udf作为装饰器包装函数来定义，不需要额外的配置。 df = spark.createDataFrame([ Row(name='A', age=20, weight=80.), Row(name='A', age=21, weight=85.), Row(name='A', age=22, weight=83.), Row(name='A', age=23, weight=78.), Row(name='B', age=30, weight=60.), Row(name='B', age=31, weight=65.), Row(name='B', age=32, weight=70.), ],schema=&quot;name string,age long,weight double&quot;) df.printSchema() df.show() root |-- name: string (nullable = true) |-- age: long (nullable = true) |-- weight: double (nullable = true) +----+---+------+ |name|age|weight| +----+---+------+ | A| 20| 80.0| | A| 21| 85.0| | A| 22| 83.0| | A| 23| 78.0| | B| 30| 60.0| | B| 31| 65.0| | B| 32| 70.0| +----+---+------+ ## 使用pandas_udf装饰器来定义udf有一下2个要点： ## 1.@pandas_udf里需要定义出参的数据格式 （pyspark.sql.types里的那些或者其对应的简写） ## 2.def的函数需定义入出参的类型，常用的有如下2种： ## ---- pd.Series -&gt; pd.Series ## ---- pd.Series -&gt; Scalar（单值，这儿需给出具体取值的类型：str、int等） @pandas_udf('long') def plus_one(series: pd.Series) -&gt; pd.Series: return series + 1 df.select(plus_one(df[&quot;age&quot;]).alias(&quot;age_plus1&quot;)).show() @pandas_udf('double') def get_mean(series: pd.Series) -&gt; float: return series.mean() df.select(get_mean(df[&quot;weight&quot;]).alias(&quot;weight_mean&quot;)).show() @pandas_udf('double') def weight2age(age:pd.Series, wt:pd.Series) -&gt; pd.Series: return (wt/age).round(2) df.select(weight2age(df[&quot;age&quot;],df[&quot;weight&quot;]).alias(&quot;wt2age&quot;)).show() +---------+ |age_plus1| +---------+ | 21| | 22| | 23| | 24| | 31| | 32| | 33| +---------+ +-----------------+ | weight_mean| +-----------------+ |74.42857142857143| +-----------------+ +------+ |wt2age| +------+ | 4.0| | 4.05| | 3.77| | 3.39| | 2.0| | 2.1| | 2.19| +------+ ## 在groupby的基础上也可以使用udf（相当于是udaf）。pd.DataFrame -&gt; pd.DataFrame ## 老版apply，通过@pandas_udf装饰器，第二个参数必须使用PandasUDFType.GROUPED_MAP @pandas_udf('name string, alpha float, beta float',PandasUDFType.GROUPED_MAP) def get_fitline_old(pdf): res = np.polyfit(pdf[&quot;weight&quot;],pdf[&quot;age&quot;],1) beta = round(res[0],2) alpha = round(res[1],2) name = pdf[&quot;name&quot;][0] return pd.DataFrame({&quot;name&quot;:[name],&quot;alpha&quot;:[alpha],&quot;beta&quot;:[beta]}) print(&quot;---- 这是老版的apply ----&quot;) df.groupby(&quot;name&quot;).apply(get_fitline_old).show() ## 新版applyInPandas，不需要装饰器，但是需要在applyInPandas里给定schema print(&quot;---- 这是新版的applyInPandas ----&quot;) def get_fitline_new(pdf): res = np.polyfit(pdf[&quot;weight&quot;],pdf[&quot;age&quot;],1) beta = round(res[0],2) alpha = round(res[1],2) name = pdf[&quot;name&quot;][0] return pd.DataFrame({&quot;name&quot;:[name],&quot;alpha&quot;:[alpha],&quot;beta&quot;:[beta]}) df.groupby(&quot;name&quot;).applyInPandas(get_fitline_new,schema=&quot;name string, alpha double, beta double&quot;).show() ---- 这是老版的apply ---- +----+-----+-----+ |name|alpha| beta| +----+-----+-----+ | A|32.74|-0.14| | B| 18.0| 0.2| +----+-----+-----+ ---- 这是新版的applyInPandas ---- +----+-----+-----+ |name|alpha| beta| +----+-----+-----+ | A|32.74|-0.14| | B| 18.0| 0.2| +----+-----+-----+ 在Spark DataFrame里操作SQL指令 针对某个spark dataframe使用createOrReplaceTempView()来进行数据库表的注册，然后通过spark.sql发起SQL操作。 同样，之前定义的pandas_udf函数可以使用在SQL查询里，通过spark.udf.register进行注册，然后在sql文里就可以使用了。 df.createOrReplaceTempView(&quot;table_tmp&quot;) spark.sql(&quot;select * from table_tmp where name='A'&quot;).show() +----+---+------+ |name|age|weight| +----+---+------+ | A| 20| 80.0| | A| 21| 85.0| | A| 22| 83.0| | A| 23| 78.0| +----+---+------+ spark.udf.register(&quot;plus_one&quot;, plus_one) spark.sql(&quot;SELECT plus_one(age) as age_plus1 FROM table_tmp&quot;).show() +---------+ |age_plus1| +---------+ | 21| | 22| | 23| | 24| | 31| | 32| | 33| +---------+ ","link":"https://lostlau.github.io/post/pyspark_tuto3/"},{"title":"PMP学习&考试 - 引子","content":"该系列博客将记录PMP的学习心得。 机缘巧合，昨晚报名了2022年9月的PMP考试。 我现有工作中，遇到最多就是“项目”，对内的项目，对外的项目，大到团队的管理，小到一次模型的refit，都可以抽象成“项目”。以前是被管理的人，经理分配任务，只需要just figure it out，然后交差，也见过一些抱怨扯皮（包括我自己）；然后自己也开始有机会带团队做项目，随着实践的项目越来越多，\b遇到的“管理”上的困难也越来越多，作为manager看到的“带团队”并不是那个作为doer看到的“带团队”。在我之后的职业规划中，应该是离不开“项目”了，所以为了能够将以后的项目做得更顺滑，我认为很有必要补充一些专业的项目管理知识。虽然我自诩是一个富有创新力、不因循守旧的人，但是我也深知presque所有的‘天马行空’也都需要站在巨人的肩膀上。 这次PMP的学习由于刚好遇到考试大纲改革，新增了非常多敏捷管理的考点，算是接地气了。因为我所遇到的项目（IT算法项目）很多和传统的项目不一样，不是以计划推动的。比如一些新的产品、新的领域的研究落地项目，可能整个团队都没有过往经验，没办法很好的预测未来各个阶段的实施内容来拟定计划，指导项目进程。类似这种情况可能就需要敏捷项目管理的方法论，刚好可以在这次改革红利中得到学习。 总之，搞起来。其他的感兴趣的事物的学习记录会继续零星的推进。 ","link":"https://lostlau.github.io/post/pmp_intro/"},{"title":"PySpark学习（二） - RDD与共享变量","content":"pyspark学习的第二篇会主要介绍RDD和相关的算子。 RDD的基本概念 在上层应用中，每个Spark应用都包含一个驱动程序，该驱动程序运行用户定义的主要功能并在集群上执行各种并行操作。Spark提供的主要数据抽象是RDD（Resilient Distirbuted Dataset: 弹性分布式数据集），它是跨集群节点分区的元素集合，可以执行并行计算。RDD是通过从Hadoop（或其支持的）文件系统中的文件开始并对其进行转换来创建的。用户还可以要求Spark将RDD持久化到内存中，以便在并行操作中有效地重用它。最后，RDD会自动从节点故障中恢复。 通俗一点的理解，RDD首先是一个dataset，即一个数据集合；然后是一个分布式的（distributed），既能分布储存，也能进行分布式计算；最后还是弹性的（resilient），表示它既能够存在硬盘中也能存在内存里，同时有较强的容错机制。 Spark中的第二个抽象是可以在并行操作中使用的共享变量。默认情况下，当 Spark 在不同节点上并行运行一个函数作为一组任务时，它会将函数中使用的每个变量的副本发送到每个任务。有时，需要在任务之间或在任务和驱动程序之间共享变量。Spark 支持两种类型的共享变量：广播变量，可用于在所有节点的内存中缓存值，以及累加器，它们是仅“添加”到的变量，例如计数器和总和。 ## 载入packages from pyspark import SparkContext from pyspark import AccumulatorParam import numpy as np import pandas as pd 初始化Spark 首先需要SparkContext创建一个和spark集群的连接，这儿我们就是本地集群，同时给应用起名tuto_rdd。 sc = SparkContext('local','tuto_rdd') sc SparkContext Spark UI Version v3.2.1 Master local AppName tuto_rdd 创建RDD 创建RDD有两种途径： 通过在驱动程序中将已有的集合并行化来创建（parallelized collection） 通过读取外部分布式数据系统，如HDFS、HBase等 ## 并行化集合可以通过SparkContext下的parallelize方法来实现 data = [1,2,3,4] rdd = sc.parallelize(data) print(f&quot;查看rdd所用分区数量：{rdd.getNumPartitions()}&quot;) 查看rdd所用分区数量：1 rdd = sc.parallelize(data,2) print(f&quot;查看rdd所用分区数量：{rdd.getNumPartitions()}&quot;) print(f&quot;查看rdd各自分区下元素集合：{rdd.glom().collect()}&quot;) 查看rdd所用分区数量：2 查看rdd各自分区下元素集合：[[1, 2], [3, 4]] ## 读取外部数据生成rdd。这儿没有分布式文件系统，就给出个example，不展开了。 rdd = sc.textFile(&quot;data.txt&quot;) RDD的操作 RDD支持两种类型的操作（算子）：Transformation（从现有数据集上创建新数据集）和Action（在数据集上进行计算后生成一个值返回给驱动程序）。比如，map将每个数据集元素通过函数传递并返回一个新RDD表示结果，是一种transformatino；而reduce使用某个函数聚合RDD的所有元素并将最终结果返回给驱动程序的操作，是一种action。 Spark中的所有转换都是惰性的，因为它们不会立即计算结果。相反，他们只记得应用于某些基础数据集（例如文件）的转换。仅当操作需要将结果返回给驱动程序时才计算转换。这种设计使 Spark 能够更高效地运行。默认情况下，每个转换操作后的（应用transformation后的）RDD可能会在我们每次对其运行操作时重新计算。但是，我们也可以使用持久化方法将RDD持久化到内存中，在这种情况下，Spark会将元素保留在集群上，下次查询时能更快地访问它。 举例： 如下代码中，前两行的代码并不会在编译的时候立马执行，而是后到最后一行的reduce触发的时候才执行，层层逆向推导。如果想保存lineLengths的结果到内存里，使用persist方法实现。 rdd_orig = sc.parallelize([1,2,3,4]) rdd_adj = rdd_orig.map(lambda s:s+1) rdd_adj.persist() res = rdd_adj.reduce(lambda a, b: a + b) res 14 transformation算子 如下记录一些基本的transformation算子和对应的代码操作 ## map:将func函数作用到数据集的每一个元素上，生成一个新的RDD返回 rdd = sc.parallelize([1,2,3,4]) rdd.map(lambda s:s+1).collect() [2, 3, 4, 5] ## filter:选出所有func返回值为true的元素，生成一个新的RDD返回 rdd = sc.parallelize([1,2,3,4]) rdd.filter(lambda s:s%3==1).collect() [1, 4] ## flatMap:先执行map的操作，再将所有对象合并为一个对象 rdd = sc.parallelize([[1,2],[3],[4,5]]) print(rdd.map(lambda l:list(np.array(l)+1)).collect()) print(rdd.flatMap(lambda l:list(np.array(l)+1)).collect()) [[2, 3], [4], [5, 6]] [2, 3, 4, 5, 6] ## mapPartitions: 同map，不过是按分区并行的，且func需要是一个iterator-&gt;iterator的映射 def sum_ptt(data_ptt): yield sum(data_ptt) def plus1_ptt(data_ptt): yield [ele+1 for ele in data_ptt] rdd1 = sc.parallelize([1,2,3,4],2) print(&quot;if there are 2 partitions：&quot;) print(rdd1.mapPartitions(sum_ptt).collect()) print(rdd1.mapPartitions(plus1_ptt).collect()) print(&quot;&quot;) print(&quot;if just 1 partition:&quot;) rdd2 = sc.parallelize([1,2,3,4],1) print(rdd2.mapPartitions(sum_ptt).collect()) print(rdd2.mapPartitions(plus1_ptt).collect()) if there are 2 partitions： [3, 7] [[2, 3], [4, 5]] if just 1 partition: [10] [[2, 3, 4, 5]] ## mapPartitionWithIndex: 同mapPartitions，但是入参带了index，（int，iterator）-&gt;iterator rdd = sc.parallelize([1,2,3,4],2) def sum_ptt_with_id(index,data_ptt): yield sum(data_ptt) def get_ptt_id(index,data_ptt): yield index print(rdd.mapPartitionsWithIndex(sum_ptt_with_id).collect()) print(rdd.mapPartitionsWithIndex(get_ptt_id).collect()) [3, 7] [0, 1] ## sample: 随机取样，无法确定具体取样的数量，参数fraction表示每个元素按古典抽样的抽样概率。我觉得用处不大 rdd = sc.parallelize(range(10),2) rdd.sample(withReplacement=False,fraction=0.5,seed=2).collect() [6, 8] ## union: 2个rdd取并，不去重 ## intersection：2个rdd取交 ## distinct: 取唯一值 ## cartesian: 取笛卡尔积，返回2个rdd的全组合排列 rdd1 = sc.parallelize([1,2,3,3],2) rdd2 = sc.parallelize([2,3,4],3) print(rdd1.union(rdd2).collect()) print(rdd1.intersection(rdd2).collect()) print(rdd1.union(rdd2).distinct().collect()) print(rdd1.cartesian(rdd2).collect()) [1, 2, 3, 3, 2, 3, 4] [2, 3] [1, 2, 3, 4] [(1, 2), (2, 2), (1, 3), (2, 3), (1, 4), (2, 4), (3, 2), (3, 2), (3, 3), (3, 3), (3, 4), (3, 4)] ## groupByKey:以第一个元素作为key进行原始rdd进行分组，返回一个新的rdd。(Key,Value)-&gt;(Key, Iterator)。 rdd = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;a&quot;,3),(&quot;c&quot;,4)],2) res = rdd.groupByKey().collect() print(&quot;元素a的结果(转化成list展示):&quot;) print(list(res[2][1])) print(&quot;原始结果&quot;) res 元素a的结果(转化成list展示): [1, 3] 原始结果 [('b', &lt;pyspark.resultiterable.ResultIterable at 0xffff6743b4f0&gt;), ('c', &lt;pyspark.resultiterable.ResultIterable at 0xffff674b7fa0&gt;), ('a', &lt;pyspark.resultiterable.ResultIterable at 0xffff674b7eb0&gt;)] ## reduceByKey：将key相同的键值对，按照Function进行计算。(Key,Value)-&gt;(Key,Value)，func 需要是(V,V)-&gt;V rdd = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;a&quot;,3),(&quot;a&quot;,4)],2) rdd.reduceByKey(lambda x,y:min(x,y)).collect() [('b', 2), ('a', 1)] ## aggregateByKey：和reduceByKey一样，不过键值对里的“取值”的出入参可以不同。 rdd = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;a&quot;,3),(&quot;a&quot;,4)],4) rdd.aggregateByKey(0,lambda x,y:x+y,lambda x,y:min(x,y)).collect() [('b', 2), ('a', 1)] ## sortByKey: 根据第一个元素排序，然后返回 rdd = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,4)],4) rdd.sortByKey().collect() [('a', 1), ('a', 4), ('b', 2), ('c', 3)] ## join: 两个数据按照第一个元素作为key的全连接 rdd1 = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,4)]) rdd2 = sc.parallelize([(&quot;a&quot;,10),(&quot;b&quot;,20),(&quot;c&quot;,30)]) rdd1.join(rdd2).collect() [('b', (2, 20)), ('c', (3, 30)), ('a', (1, 10)), ('a', (4, 10))] ## coalesce: 把rdd的分区数量降低至指定数量 rdd = sc.parallelize([1,2,3,4],4) rdd.coalesce(2).glom().collect() [[1, 2], [3, 4]] action算子 如下记录一些基本的action算子和对应的代码操作 ## collect：将rdd里的信息返回成一个list，存在内存里 rdd = sc.parallelize([1,2,3,4]) rdd.collect() [1, 2, 3, 4] ## reduce: 将rdd中元素两两传递给输入函数，同时产生一个新的值，新产生的值与RDD中下一个元素再被传递给输入函数直到最后只有一个值为止。 ## 相当于输入函数需要满足交换律和结合律（i.e. f(a,b)=f(b,a) &amp; f(f(a,b),c)=f(a,f(b,c))） rdd = sc.parallelize([1,2,3,4],4) rdd.reduce(lambda x,y:min(x,y)) 1 ## count: 计数 rdd = sc.parallelize([1,2,3,4],4) rdd.count() 4 ## take: 返回前n个元素 ## takeSample: 随机取n个元素 ## takeOrdered: 先排序再返回前n个元素 rdd = sc.parallelize([1,2,4,3],4) print(rdd.take(2)) print(rdd.takeSample(withReplacement=False,num=2,seed=1)) print(rdd.takeOrdered(3)) [1, 2] [3, 1] [1, 2, 3] ## coutByKey: 根据key（第一个元素）分组计数 ## countByValue: 根据value（第二个元素）分组计数 rdd = sc.parallelize([(&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,4)]) print(rdd.countByKey()) print(rdd.countByValue()) defaultdict(&lt;class 'int'&gt;, {'a': 2, 'b': 1, 'c': 1}) defaultdict(&lt;class 'int'&gt;, {('a', 1): 1, ('b', 2): 1, ('c', 3): 1, ('a', 4): 1}) 共享变量 通常情况下，当上述介绍的这些Spark算子在远程集群节点上执行时，它中间调用的函数所使用的变量是在单独副本上工作的，这些变量将被复制到每台机器/节点上，并且对远程机器上的变量的更新不会传播回驱动程序。支持跨任务的通用读写共享变量效率非常低，于是Spark确实为两种常见的使用模式提供了两种有限类型的共享变量：broadcast variable和accumulator。 p.s.以下只是简单的说一下我对这两个共享变量的理解，我觉得这些主要是在资源调度上的考虑，目前学习中还没有真正弄懂其中的奥妙，可能需要在实际实践中体会 broadcast variable 简单的说，broadcast variable相当于是在每台机器上缓存的一个只读变量，而不用随任务一起发送它的副本，目的是为了降低通信成本。 它的使用是通过SparkContext下的broadcast方法去封装原始数据信息: brdvar = sc.broadcast([1,2,3,4]) brdvar.value [1, 2, 3, 4] accumulator accumulator是一个满足交换律和结合律的只“加”变量（此处“加”可视为一个抽象的映射，和reduce算子中的func的所需的特性一样），因此可以有效地并行支持。它可用于实现计数器或求和，Spark原生支持数值类型的accumulator，使用的时候可以自定义添加对新类型的支持。 accumulator通过SparkContext下的accumulator方法来创建，需要传入一个初始值。accumulator同样满足Spark的惰性计算，即需要action来触发结果的收集返回。 同时，如果想要自定义不同类型的accumulator，可以通过传入accum_param来实现，accum_param可以通过继承AccumulatorParam来创建。AccumulatorParam里面需要定义2个方法：zero和addInPlace。前者用来定义0值类型，后者用来定义抽象“加”法的计算逻辑。 accum = sc.accumulator(0) def f(x): accum.add(x) return x sc.parallelize([1,2,3,4],3).map(f) print(f&quot;1.仅进行transformer，accum={accum}&quot;) sc.parallelize([1,2,3,4],3).map(f).collect() print(f&quot;2.进行action，accum={accum}&quot;) 1.仅进行transformer，accum=0 2.进行action，accum=10 class vec_accumparam(AccumulatorParam): def zero(self, v): return [0]*len(v) def addInPlace(self, v1, v2): try: return [v1[i]+v2[i] for i in range(len(v1))] except: return [v1[i]+0 for i in range(len(v1))] accum_vec = sc.accumulator([1,2,3,4], vec_accumparam()) accum_vec.add([2,2,2,2]) accum_vec.value [3, 4, 5, 6] class str_accumparam(AccumulatorParam): def zero(self, v): return &quot;&quot; def addInPlace(self, v1, v2): return v1+v2 accum_str = sc.accumulator(&quot;a&quot;, str_accumparam()) accum_str.add(&quot;b&quot;) accum_str.value 'ab' ","link":"https://lostlau.github.io/post/pyspark_tuto2/"},{"title":"PySpark学习（一） - 学习环境的配置","content":"之前有做过一些大数据工程的项目，由于spark不是我的技术栈范围，所以取巧用了hive-udf编译的python脚本。想来spark是未来数据处理的趋势，所以最近开始学习pyspark，这个系列的博客将会边学边记录，可能会比较囫囵吞枣。第一篇会记录pyspark学习所需要的环境配置。 PySpark环境设置 一般的普罗大众想学习Spark，很难自己去配置好整个Spark集群和套件等相关运行环境，这样成本太高，入门难度也太大。受益于如今容器化技术（比如Docker）的发展，学习新技术的环境配置门槛被大幅降低，我们可以通过官方社群中所提供的已经预配置好的Docker镜像（Image）来模拟整个学习研究环境，我们只需要专注于想学的技术核心本身，而不用花大把精力去搞研究环境的搭建。 对于对容器化技术不了解的朋友，可以把它简单的理解为一个和本机相对隔离的环境，在这个环境里使用的工具包或者跑的程序的依赖工具也是用的这个环境里配置好的，而不用使用这个环境外的（比如主机自带的）。这样可以解决不同程序所需要的依赖的同一个package的版本不同的问题，使得耦合性很低，在不用的环境里运行不同的程序或者做不同的研究，相互不影响。就像在做开发的时候在pycharm里面每个project都需要单独配置一个虚拟环境，道理是类似的。 同样的，PySpark作为一个python中蓬勃向上的大数据工程类工具package，Docker Hub也有其对应的镜像。为了能方便在Jupyter中使用，Jupyter Docker Stacks提供了pyspark-notebook的Docker镜像。其实我也是第一次接触Jupyter Docker Stacks，我看了他们的文档，发现贼好用，相当于jupyter为了教学方面考虑，提供了一些重要技术工具的研究学习环境，开源共享在Docker Hub上面。目前提供的镜像还不是很多，比较有用的是：科学计算scipy相关的，神经网络tensorflow相关的和分布式计算pyspark相关的。 言归正传，要运行docker镜像，先需要下载一个docker，Mac电脑的可以直接通过homebrew安装（需要APP客户端的记得带--cask，不然就只能命令行操作了）。有了docker之后，cd到对应的研究文件夹下面，运行如下命令行指令可以运行jupyter-pyspark镜像。 $ docker run -it -p 9000:8888 -v $PWD:/home/jovyan jupyter/pyspark-notebook 上述指令-p 9000:8888代表将送往localhost:9000的通讯转送至Docker container的8888端口(也就是这个容器里Jupyter notebook的地址）；而-v $PWD:/home/jovyan代表将本机当前文件夹挂载至Docker container内的路径/home/jovyan上。这儿9000可以改成其他的你想要的端口号，但是我不建议改成8888，因为8888是默认的jupyter notebook的端口号，你如果想用本机环境运行jupyter notebook的话，就需要在启动jupyter的时候单独指定其他的端口号防止跳到pyspark-notebook环境里去。jovyan是Jupyter Docker Stack的预设使用者名字，所以按它预设的文件夹名称，不用过多在意。如果想给这个容器定制命名为pyspark_tuto的话可以在run后面加上--name=&quot;pyspark_tuto&quot;来实现。同时，如果之后想查看Spark UI，需要将4040端口也映射到本地，即在-p 9000:8888后面加上 -p 4040:4040（前面那个4040可以换成你想要的端口号）。 对于已经创建好的docker container如果想增加端口映射的话，可以参考这篇《Mac下 Docker 动态添加端口》。 上述运行成功的话，那么可以在Docker客户端里看到对应的容器： 在浏览器里面输入localhost:9000就可以打开jupyter-lab的界面，如果需要token，那么可以在日志log里面找到： PySpark基本操作 以下介绍一些基本的pyspark语法，首先载入packages依赖 import pandas as pd import numpy as np from pyspark.sql import SparkSession import warnings warnings.filterwarnings('ignore') PySpark模块基本概念 pyspark里最核心的模块是SparkContext（简称sc）,最重要的数据载体是RDD。 SparkContext是spark的主要切入点，而RDD是主要的API，所以需要通过sparkcontext来创建和操作RDD。对于不同的API，我们需要使用不同的context，比如对于streming我们需要使用StreamingContext；对于sql，使用sqlContext；对于hive，使用hiveContext。不过目前业界普遍使用DataSet和DataFrame的API逐渐成为标准API，所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切人点。其实质也是sqlContext和hiveContext的组合（streamingContext还未实现），所以在sqlContext和hiveContext上可用的API在SparkSession上同样可以使用。SparkSession内部封装了SparkContext，计算上其实也是由SparkContext完成的。 spark = SparkSession.builder.getOrCreate() 读取数据 以下使用样例数据https://www.kaggle.com/aashita/nyt-comments 可以采用两种方法读取，一种是spark自带的read，另一种是通过转换pandas.DataFrame的方式 df = spark.read.option('header',True).option('escape','&quot;').option('inferSchema',True).csv(&quot;archive/CommentsFeb2017.csv&quot;) df.printSchema() root |-- approveDate: string (nullable = true) |-- articleID: string (nullable = true) |-- articleWordCount: string (nullable = true) |-- commentBody: string (nullable = true) |-- commentID: string (nullable = true) |-- commentSequence: string (nullable = true) |-- commentTitle: string (nullable = true) |-- commentType: string (nullable = true) |-- createDate: string (nullable = true) |-- depth: string (nullable = true) |-- editorsSelection: string (nullable = true) |-- inReplyTo: double (nullable = true) |-- newDesk: string (nullable = true) |-- parentID: string (nullable = true) |-- parentUserDisplayName: string (nullable = true) |-- permID: string (nullable = true) |-- picURL: string (nullable = true) |-- printPage: integer (nullable = true) |-- recommendations: integer (nullable = true) |-- recommendedFlag: string (nullable = true) |-- replyCount: string (nullable = true) |-- reportAbuseFlag: string (nullable = true) |-- sectionName: string (nullable = true) |-- sharing: integer (nullable = true) |-- status: string (nullable = true) |-- timespeople: string (nullable = true) |-- trusted: string (nullable = true) |-- updateDate: string (nullable = true) |-- userDisplayName: string (nullable = true) |-- userID: double (nullable = true) |-- userLocation: string (nullable = true) |-- userTitle: string (nullable = true) |-- userURL: string (nullable = true) |-- typeOfMaterial: string (nullable = true) import pandas as pd import numpy as np df_pd = pd.read_csv(&quot;archive/CommentsFeb2017.csv&quot;) df_prime = spark.createDataFrame(df_pd.astype(str)) df_prime.printSchema() root |-- approveDate: string (nullable = true) |-- articleID: string (nullable = true) |-- articleWordCount: string (nullable = true) |-- commentBody: string (nullable = true) |-- commentID: string (nullable = true) |-- commentSequence: string (nullable = true) |-- commentTitle: string (nullable = true) |-- commentType: string (nullable = true) |-- createDate: string (nullable = true) |-- depth: string (nullable = true) |-- editorsSelection: string (nullable = true) |-- inReplyTo: string (nullable = true) |-- newDesk: string (nullable = true) |-- parentID: string (nullable = true) |-- parentUserDisplayName: string (nullable = true) |-- permID: string (nullable = true) |-- picURL: string (nullable = true) |-- printPage: string (nullable = true) |-- recommendations: string (nullable = true) |-- recommendedFlag: string (nullable = true) |-- replyCount: string (nullable = true) |-- reportAbuseFlag: string (nullable = true) |-- sectionName: string (nullable = true) |-- sharing: string (nullable = true) |-- status: string (nullable = true) |-- timespeople: string (nullable = true) |-- trusted: string (nullable = true) |-- updateDate: string (nullable = true) |-- userDisplayName: string (nullable = true) |-- userID: string (nullable = true) |-- userLocation: string (nullable = true) |-- userTitle: string (nullable = true) |-- userURL: string (nullable = true) |-- typeOfMaterial: string (nullable = true) 基于DataFrame操作数据 df.select([&quot;articleID&quot;,&quot;printPage&quot;]).limit(1).collect() [Row(articleID='58927e0495d0e0392607e1b3', printPage=12)] df.createOrReplaceTempView(&quot;comment&quot;) spark.sql(&quot;select commentBody, printPage from comment limit 5&quot;).show(truncate=True,vertical=False) +--------------------+---------+ | commentBody|printPage| +--------------------+---------+ |ANY anti Trump pr...| 12| |I'll not watch th...| 12| |NFL's going to do...| 12| |I'm continually a...| 12| |Personally, I do ...| 12| +--------------------+---------+ spark.sql(&quot;select commentBody, printPage from comment limit 5&quot;).toPandas() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } commentBody printPage 0 ANY anti Trump propaganda from Gaga and my TV ... 12 1 I'll not watch the SB, nor the grammys or osca... 12 2 NFL's going to do another \"in-your-face, Ameri... 12 3 I'm continually amazed at the ill-placed crede... 12 4 Personally, I do not want to see any politics ... 12 ","link":"https://lostlau.github.io/post/pyspark_tuto1/"},{"title":"探索树莓派 - Let the hunt begin","content":"最近突然对树莓派（Rapsberry Pi）感兴趣，了解到它是一块类似于小型单片机的主板，可以做服务器，也可以通过智能自动化编程将软硬件结合搞点新东西。所以买了一个尝试自己摸索学习一下。 树莓派购买 第一步，肯定是先购买一个树莓派，选购一条龙可参考这儿。当前最新的树莓派是4 modelB，我在京东上买的（因为可以第二天就收货，等不及了XD），同时购买相关的基础套件，比如亚克力保护盒子、散热片、风扇、网线、SD卡等（如果是macbook，最好买一个typec的多功能接口转换器，USB、HDMI、SD的读取会用到的）。很多商家都有打包的套餐，也可以选择散买。我主板是在商家MAKEBIT买的，散件是在隽鹏数码（JUNROC）买的。JUNROC是树莓派官方指定的经销商，不过最近主板非常紧俏，已经莫得了，只能买点散件。说到这个，我买的4B带4G内存的要1000出头，不知道是不是国内树莓派被炒得太火了，比官方指导价贵了接近1倍。 硬件组装 我买的都是基础的官方配件，组装不多说了，动动手和脑袋。就补充2点： 散热片贴在这3个地方，看如下图： 小风扇安装如下图（正反其实无所谓），关键是红黑两根线要接到正确的位置上。 系统烧录 * P.S.我个人电脑用的是MacBookPro13，所以如下的一些指令操作主要是基于MacOS系统的。 系统烧录推荐用Raspberry Pi Imager，官方出品，直接下载到指定的SD卡上，很方便。 先下载RPI，可以直接通过homebrew下载： brew install --cask raspberry-pi-imager 把SD卡（推荐16G以上）插到电脑上。Mac可能无法识别或者无法全部识别SD卡（通过访达边栏或者磁盘工具可查看外接SD是否挂载上和挂载上的空间大小，如果识别的空间大小和SD卡原始大小差距很大，说明没有全部挂载上），这大概率是因为SD卡的格式问题，需要把整个SD卡格式化成Mac可识别的格式（比如FAT32）。 方法是在terminal终端里使用Mac自带的diskutil工具进行格式化。具体细节可以参考这篇CSDN博客。 其实我个人觉得拿到SD卡都可以先格式化一下，确保后续操作的安全:)。 然后，通过RPI进行烧录。 进入软件： 选择需要写入的操作系统，没特殊需要可以直接用第一个官方推荐： 选择外接的储蓄卡，准备烧录。 前方高能！点击“烧录”前，右下角有个设置按钮，点进去，里面有一些系统的高级设置，可以直接在这里设置了。 如果这儿不设置，后续烧录完了也能进图形化界面里去设置。但是我推荐在这儿设置，因为一个是统一，二一个是可以避免后面可能出现的坑爹场景（比如没有图形化界面的情况下想要通过笔记本SSH远程连接进去，比如默认的用户名密码跟网上攻略说的不一样，或者单独去进行连网的一些设置可能踩的坑）。这些踩坑场景大多数出现在没有图形显示器的情况下，然后刚好树莓派的配置项更新换代导致网上攻略教程过时😭。 系统登入 烧录好了之后，把SD卡插入树莓派底座，开机！ 有显示屏幕，可以直接接上去，就有图形画面了： 如果没有显示屏，那么根据烧录时候设置的wifi和密码，树莓派会接入网络，让个人Mac笔记本也处于同一wifi下，然后通过之前烧录时设置的SSH，通过个人Mac在终端输入命令远程连接： ssh pi@raspberry.local 其中pi是之前设置的username，raspberry.local是之前设置的主机名。树莓派IP可以在wifi路由器管理界面看到，比如TP-link是tplogin.cn，进去后输入密码（默认是123456），点击设备管理，可以看到raspberrypi设备的IP，比如我的是192.168.0.106，这个IP地址也可以替换raspberry.local，变为 ssh pi@192.168.0.106 登陆需要密码，输入烧录时候的密码，连接上去，相当于现在可以通过命令行远程操控树莓派了。 为了可以远程通过图形化手段操作树莓派，我们需要继续通过命令行去更改一些配置，输入： sudo raspi-config 进入设置界面，选择 3 Interface Options -&gt; I3 VNC，启用VNC（一款图形化远程连接工具）交互： 然后同时可以去 2 Display Options -&gt; D5 VNC Resolution 把分辨率调到1920x1080（省得之后再回来调）。设置完了之后，下载VNC，连接就可以通过图形化界面远程连接进入了！和用显示器的效果一样。 如果，很不幸地，烧录的时候没有进行高级设置，又不想重新烧录且又没有显示器（刚好是我的情况😄），那么必须得使用网线使得树莓派和你的个人Mac在同一局域网下，然后建议阅读如下几篇参考文章，来最终实现远程VNC的连接访问： SSH&amp;VNC连接-参考1 SSH&amp;VNC连接-参考2 新版raspbian系统默认用户名和密码的变化导致ssh的permission-denied问题 ","link":"https://lostlau.github.io/post/raspberrypi_1/"},{"title":"Markdown 简明语法参考","content":"详细介绍了markdown的语法，算是工具书。 Markdown 概述 Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。 —— 维基百科 Markdown 语法是对纯文本格式的强化，能使文本显示得更清晰、有条理。但它依旧算是简单的文本，很容易修改和扩展，常用于快速写作中。 所见即所得。不少 MD 编辑器有极强的实时预览渲染，可以让写作者及时看到自己写作的内容显示效果并对此做出调整。 Markdown 格式的简洁特性、兼容扩展性颇佳，使之能快速转换为各种互联网上的常用格式，比如 HTML、Word、PDF 等。目前，越来越多的人开始接受和使用它。 Markdown 编辑器 常见及常用的 Markdown 编辑器很多，比如： MacDown Typora 此外还有（收费为主）： Scrivener Typed Byword Marked 2 Ulysses Marboo Mou 以及支持 Markdown 的优质在线编辑器： 马克飞象 Dillinger StackEdit Markable 入门编辑器推荐第一序列的 MacDown，其它编辑器可以等熟悉 Markdown 之后再自行探索。 Markdown 语法 关于 Markdown 的基本语法，这里将逐一说明。对于一些写作中一般不算常用（不好用）的表格及代码展示，不多说明。 基本所有以下涉及的标点都以在英文输入状态下的为准。不然会导致语法失效。 先看一个包含了蛮多基本语法的范例。 关于语法，让我们先从在文中起到分清区块、梳理逻辑关系的标题说起： 标题 # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 显示效果如下： 这是一级标题 俺，二级标题 我是三级标题 人家只是个四级标题…… 更小的我，五级标题 XD Github 支持的最小的可以我（哪里冒出来的） 换行 Markdown 语法下，换行的方式有： 隔行换行。连续敲击两下「Return」键，再开始写下一段； 在行末添加两个空格符，而后「Return」（为了显示清晰），开始写下一段。 强调标记 *斜体* 斜体 _斜体_ **加粗** 加粗 __粗体_ 对于强调符号，需要注意的就是，以什么开始，以什么结尾，数量也保持一致，相应的语法才能生效。 显示效果如下： 这是 斜体 这是 强调 分割线 Partition Line *** 加分割线 * * * 加分割线 ***** 或者这样？ --- 还有另外的方式 ___ 其实，还可以这样 以下就是一条分割线： 列表 列表分无序列表合有序列表两类，前者以「*」、「-」、「+」开头，后者以阿拉伯数字「1」开头。 三个注意点： 如果前面有内容，在输入字符前，应先空一行； 符号与内容之间，须隔一个空格符，列表模式才能生效； 第一行内容输入完成，换行会自动补全下一行的开头符号（有序列表数字递增，无序列表符号不变），接下去只需继续输下一行内容，以此类推； 多层级列表，为美观也为减少出错考虑，注意同级列表符号的统一和上下对齐。 无序列表： Unordered List: * English * Japanese * Chinese * …… Unordered List: - English - Japanese - Chinese - …… 显示效果均如下： Country List： English Japanese Chinese …… 有序列表： Ordered List: 1. Orange 2. Apple 3. Banana My Favorite Fruit List: Orange Apple Banana 多层级列表： 两都名胜 - 南京 + 玄武湖 + 中山陵 + 鸡鸣寺 - 杭州 + 西湖 * 苏堤 * 湖心亭 * 太子湾 + 九溪 + 灵隐 注：此处的几个符号作用都是一样的，不同层级符号有所区别只为显示美观。日常使用，请每一级的符号一致。 显示效果如下： 两都名胜 南京 玄武湖 中山陵 鸡鸣寺 杭州 西湖 苏堤 湖心亭 太子湾 九溪 灵隐 引用 Quote 引用。使用「&gt;」，添加在每行的开头。 两个注意点： 「&gt;」的上一行，必须为空行； 「&gt;」与其后紧跟的第一个字之间有无空格符不影响效果（不过建议加一个）。 显示效果如下： 詹姆斯·馬奇《馬奇論管理》： 堂吉诃德提醒我们，如果我們只在不被辜負時去信任，只在有所回報時去愛，只在學有所用時去學習，那麼我們就放棄了為人的特征——願意在自我理念的名義下行動，不管結果如何。 或者，你想要引用一首诗、一首歌： &gt; 五月天《天使》 &gt; 像孩子依赖着肩膀 &gt; 像眼泪依赖着脸庞 &gt; 你就像天使一样 &gt; 给我依赖 给我力量 这儿也需要注意换行：每一行最后换行，添加两个空格符，不然内容会挤压在一块，变成一行。或者行与行之间，空一行。以下例子中，歌词部分输入时虽然都分列开，而实际显示时歌词都连成一句了。 五月天《天使》 像孩子依赖着肩膀 像眼泪依赖着脸庞 你就像天使一样 给我依赖 给我力量 每句歌词的行尾空两格之后，效果是这样的： 五月天《天使》 像孩子依赖着肩膀 像眼泪依赖着脸庞 你就像天使一样 给我依赖 给我力量 如果是想嵌套引用，像这样： 浙江 杭州 西湖 湖心亭 博客默认的 Markdown 工具有点问题，效果如下图： 语法如下，每行多添加一个「&gt;」符号，再输入文字内容即可（每行行尾记得添两个空格符）： &gt; 浙江 &gt;&gt; 杭州 &gt;&gt;&gt; 西湖 &gt;&gt;&gt;&gt; 湖心亭 代码区 也就是 Blockquotes。 如果行尾不空格呢有没其它解决方法呢？如果是这样—— 风吹柳絮 《麦兜当当伴我心》插曲 风吹柳絮 茫茫难聚 随着风吹 飘来飘去 我若能够携你随风去 我愿像一块扣肉 我愿像一块扣肉 我愿像一块扣肉 扣住你梅菜扣住你手 我愿像一块扣肉 我愿像一块扣肉 我是你一块扣肉 你是那梅菜扣住你手 这里的方法是首行开头缩进四个空格符，或敲一下 tab 键（制表符键）。 还有一个方法。连续三个反引号「`」组成的前后两行，将内容包裹起来。 显示效果如下： 这是另一个代码区块 在代码区块中，Markdown 语法不会被转换，这也是为什么前面很多 Markdown 语法的例子能在代码区展示出来的缘故。不然这样一篇以 Markdown 语法解释 Markdown 语法的说明也无从谈起了。 嗯...嗯？ 反引号的输入：英文输入模式下，点击键盘左上角的「~」键。 标记 标记小段代码（文字）。为着重强调的内容添加深色背景框，在内容前后各添一个反引号「`」，将代码段或文字夹在中间即可实现。 显示效果如下： Use the printf() function. 这是加深背景色框的字符。 网址链接 自动链接 使用「&lt;」、「&gt;」这样的尖角符号，url/email 在 Markdown 下可自动实现可点击链接的效果。 &lt;http://www.google.com&gt; 显示效果如下： http://www.google.com 网址链接 至于网址链接的基本格式，应该是这样： [Link Name](Link) 构成为： 一个方括号，添加图片的描述文字 一个括号，添加图片网址 以下为一个网址的栗子： [Welcome to my blog](http://azeril.me) 显示效果如下： Welcome to my blog 索引链接 内容描述后添加定义链接（以数字/英文/符号为主），在文字段落外关联具体网址，实现可跳转效果。 [Click Google Search][Tags] [Tags]: http://www.google.com &quot;Google&quot; 显示效果如下： Click Google Search 添加图片 插入图片的语法： ![Pic name](Pic link) ![Instagram Pic](http://i.imgur.com/UKhrRrK.jpg) 一个英文输入下的惊叹号「!」； 一个方括号，添加图片的描述文字； 一个括号，添加图片网址。 相比插入网址链接，多了一个开头的惊叹号。 显示效果如下： 添加表格 |Title |Title 1|Title 2|Title 3| |---|---:|:---:|---:| | A|B|C|D| 显示效果如下： Province ZJ 浙江省 FJ 福建省 YN 云南省 省会 杭州 厦门 昆明 一般的表格由「|」与「-」两种符号（英文半角字符）构成。第一、三及其后的行都由「|」组成。依数据的列数确定数量（列数据量 +1）。 第二行为中间为连续的「-」组成的隔断，数量不限，更多是让文本显得美观（和预览无关）。 第二行中出现的冒号作用是设定表格内数据的对齐方式，不是必须使用的。具体意义如下： :–-- 冒号在左，左对齐 --–： 冒号在右，右对齐 ：--–： 左右两侧都出现冒号，居中对齐 转义符 如何在 MD 文档中输出被用于转换格式的符号本身？这里就需要转义符，也就是反斜线「\\」来协助。 如果要显示「*」，则可以用如下的方式： 表示强调的符号这样用： \\*Emphasize\\* 显示效果如下： 表示强调的符号这样用： *Emphasize* 支持转义的 MD 符号包括： \\ 反斜线 ` 反引号 \\* 星号 \\_ 下划线 {} 花括号 [] 方括号 () 圆括号 # 井号 + 加号 - 减号（连字符） . 句点 ! 感叹号 扩展阅读 基于 Markdown 的 HTML 语言运用。 HTML 可以契合 MD 语法，而通过利用前者，可以实现一些单纯依靠 MD 语法暂时无法实现的功能和页面显示效果。 网址链接 页面内跳转链接。利用 HTML 语法制作 Markdown 长文的可跳转目录。分两部分，前为具体条目信息，后边则指向内容的位置（代码段放在页面的哪里，点击索引条目后就跳转到哪里）。 范例语法如下： [Line](#A) &lt;a name=&quot;A&quot;&gt;&lt;/a&gt; Line 添加图片(进阶版) 在页面中，要并排插入多张图片（两张或三张并排显示）或将插入图片居中显示，又或者是类似固定显示图片的宽度与高度，目前依靠 Markdown 语法还无法实现。 这时，调用 HTML 语法就可以轻松搞定。 以下，可以仅看添加图床图片的那部分，减少认知负荷。 图片与图床 图床是第三方服务托管个人上传图片，并提供图片外链，让我们在写博客文章时使用的方式。 常用图床： 七牛云存储 Imgur Photobucket 图床在上传图片后会提供一个图片的外链。我们可以利用外链把图片添加到自己的博文中。图片也可以存储在 Github 自己的博客仓库里，不过图片蛮占用空间（Github 项目空间为200 M）。因而最好是选图床托管图片。 如果是单纯插入来自图床的图片外链： 单张居中显示： &lt;center&gt; &lt;img src=&quot;http://dreamofbook.qiniudn.com/Zero.png&quot;&gt; &lt;/center&gt; OR &lt;figure&gt; &lt;img src=&quot;http://xxx.jpg&quot;&gt; &lt;/figure&gt; 效果如下： 固定图片宽度/高度： &lt;img src=&quot;http://xxx.jpg&quot; title=&quot;Logo&quot; width=&quot;100&quot; /&gt; 宽度是 Width，高度是 High。Title 为图片描述。 效果如下： 两张并排显示： &lt;figure class=&quot;half&quot;&gt; &lt;img src=&quot;http://xxx.jpg&quot;&gt; &lt;img src=&quot;http://yyy.jpg&quot;&gt; &lt;/figure&gt; 三张并排显示： &lt;figure class=&quot;third&quot;&gt; &lt;img src=&quot;http://xxx.jpg&quot;&gt; &lt;img src=&quot;http://yyy.jpg&quot;&gt; &lt;img src=&quot;http://zzz.jpg&quot;&gt; &lt;/figure&gt; 使用时复制相应的代码粘贴，然后替换 &quot;&quot;(英文输入下的引号) 里的链接，也就是例子中的「http://xxx.jpg」链接为自己上传图片（一般只有在图床上传的图才有这样的链接）的外链。 Tips: 关于 jpg/png 都是常用文件格式，jpg 格式压缩效率高，相对文件质量小一点，占用网络空间少，在页面里显示时加载会快一些。png 格式的图片更清晰。 添加 Github 图片 如果是利用在 Github 项目文件夹里的图片，基本的语法不变，只是将前面提到 &quot;&quot; 里的图片外链（网址）替换为： 「自定义的域名 + 图片在 Github 项目的位置」 {{ site.url }}/images/xxx.jpg 引用的代码自动变形了。博文中两个花括号及「site.url」构成的代码块会自动补全为我的个人域名，所以这篇博文代码区中的演示也受到了影响(上一段的引用就受到影响了。后边的文中引用的例子也一样)。如果复制了刚才的代码块，需要将「 azeril.me//」修改成下图中的样子（两个花括号，中间加 site.url ）。即： 使用时复制和修改相应的代码，并替换「/images/xxx.jpg」这样的链接为自己放置图片的路径。 如果 Github 项目下有 images 文件夹，直接在里面放图，就是如上的代码替换。如果是 images 目录下的文件夹，如 Instagram，则路径为：「images/Instagram/ xxx.jpg」。依此类推。 附加浏览窗口的模式 更复杂一点，点击图片可以跳出一个预览窗口，可以看大图和切换图片。 效果如下： 两张并排显示的代码（单张和三张代码和前述的类似）： &lt;figure class=&quot;half&quot;&gt; &lt;a href=&quot;{{ site.url }}/images/xxx.jpg&quot;&gt;&lt;img src=&quot;{{ site.url }}/images/ xxx.jpg&quot;&gt;&lt;/a&gt; &lt;a href=&quot;{{ site.url }}/images/yyy.jpg&quot;&gt;&lt;img src=&quot;{{ site.url }}/images/ yyy.jpg&quot;&gt;&lt;/a&gt; &lt;/figure&gt; 添加图片代码分享 因博客显示时引用的部分 HTML 代码有问题，所以另外发一份 md 版本的文档供参考。 下载地址： 百度云 - Dl from BaiduYun box.com - Dl from Box.com dropbox - Dl from Dropbox 愉快。 参考文档及扩展阅读 献给写作者的 Markdown 新手指南_简书 Markdown 语法说明_WowUbuntu Markdown 语法说明（详解版）_图灵社区 Mastering Markdown · GitHub Guides Markdown - Wikiwand Markdown 写作浅谈 - 阳志平的网志 ","link":"https://lostlau.github.io/post/markdown_tuto/"},{"title":"关于","content":"鉴于本人拖延症，“关于我”决定有空再补。😅 ","link":"https://lostlau.github.io/post/about/"}]}