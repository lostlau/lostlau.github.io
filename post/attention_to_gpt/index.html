<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8" />
<title>
    Lost Blog
</title>
<!--[if lt IE 9]><script src="//cdn.bootcss.com/html5shiv/r29/html5.js"></script><![endif]-->
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport" />
<meta http-equiv="x-dns-prefetch-control" content="on">
<link rel="dns-prefetch" href="https://fastly.jsdelivr.net">
<meta name="author" content="刘蔚南">
<meta name="description" content="Un jour, j&#39;ai eu son âge.">
<meta name="keywords" content="lost,刘蔚南">
<script async src="//instant.page/3.0.0" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>
<link rel="stylesheet" href="https://lostlau.github.io/styles/main.css" />
<link href="https://fastly.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" rel="stylesheet">
<script type="text/javascript" src="https://apps.bdimg.com/libs/jquery/1.7.1/jquery.min.js"></script>

        <link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/itjoker233/Gridea-theme-Chic/assets/media/css/style.min.css" />
        <script src="https://fastly.jsdelivr.net/gh/itjoker233/Gridea-theme-Chic/assets/media/script/tocbot.min.js"></script>
        <script src="https://fastly.jsdelivr.net/gh/itjoker233/Gridea-theme-Chic/assets/media/script/script.min.js"></script>
        <script src="https://fastly.jsdelivr.net/gh/itjoker233/Gridea-theme-Chic/assets/media/script/icon.min.js"></script>
        
            <script src="https://fastly.jsdelivr.net/gh/ITJoker233/ITJoker233.github.io@latest/CDN/js/Card/prism.min.js"></script>
            <link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/ITJoker233/ITJoker233.github.io@latest/CDN/css/Card/prism.min.css" />
            <link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css">
            <script defer src="https://fastly.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js"></script>
            
                        <!--CDN样式-->
                        <script src="https://fastly.jsdelivr.net/gh/ITJoker233/ITJoker233.github.io@latest/CDN/js/hit-kounter-lc-0.3.0.js"></script>
                        <script src="https://cdn1.lncld.net/static/js/av-min-1.5.0.js"></script>
                        
                            <script>
                                (function() {
                                    var bp = document.createElement('script');
                                    var curProtocol = window.location.protocol.split(':')[0];
                                    if (curProtocol === 'https') {
                                        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
                                    } else {
                                        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
                                    }
                                    var s = document.getElementsByTagName("script")[0];
                                    s.parentNode.insertBefore(bp, s);
                                })();
                            </script>
                            
                                <script async src="https://www.googletagmanager.com/gtag/js?id=G-FCPT77VC7L"></script>
                                <script>
                                    window.dataLayer = window.dataLayer || [];

                                    function gtag() {
                                        dataLayer.push(arguments);
                                    }
                                    gtag('js', new Date());
                                    gtag('config', 'G-FCPT77VC7L');
                                </script>
                                
                                    <script type="text/javascript">
                                        var _hmt = _hmt || [];
                                        (function() {
                                            var hm = document.createElement("script");
                                            hm.src = "https://hm.baidu.com/hm.js?46a94dcfca01e26a201e1f91f69a452f";
                                            var s = document.getElementsByTagName("script")[0];
                                            s.parentNode.insertBefore(hm, s);
                                        })();
                                    </script>
                                    
</head>

<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo">
                <a href="https://lostlau.github.io">
                    Lost Blog
                </a>
            </div>
            <div id="tp-weather-widget"></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/">
                        首页
                    </a>
                    
                    <a class="menu-item" href="/post/about">
                        关于我
                    </a>
                    
                    <a class="menu-item" href="/archives">
                        文章
                    </a>
                    
                    <a class="menu-item" href="/tags">
                        标签分类
                    </a>
                    
                        <input id="switch_default" type="checkbox" class="switch_default">
                        <label for="switch_default" class="toggleBtn"></label>
            </div>
            <form id="gridea-search-form" data-update="1753006403576" action="/search/index.html">
                <input class="search-input" autocomplete="off" spellcheck="false" name="q" autofocus="true" placeholder="Search...">
            </form>
        </div>
    </nav>

    
        <nav class="navbar-mobile" id="nav-mobile">
            <div class="container">
                <div class="navbar-header">
                    <div>
                        <a href="https://lostlau.github.io">
                            Lost Blog
                        </a>
                        <!--en-->
                        <a id="mobile-toggle-theme-en" class="a en">&nbsp;Dark</a>
                        <!--zh-->
                        <a id="mobile-toggle-theme-zh" class="a zh">&nbsp;&#x6697;&#x9ED1;</a>
                    </div>
                    <form id="gridea-search-form" data-update="1753006403576" action="/search/index.html">
                        <input class="search-input" autocomplete="off" spellcheck="false" name="q" autofocus="true" placeholder="Search...">
                    </form>
                    <!--en-->
                    <div class="menu-toggle" id="menu-toggle-en" onclick="mobileBtn()">&#9776; Menu</div>
                    <!--zh-->
                    <div class="menu-toggle" id="menu-toggle-zh" onclick="mobileBtn()">&#9776; &#x83DC;&#x5355;</div>

                </div>
                <div class="menu" id="mobile-menu">
                    
                        <a class="menu-item" href="/">
                            首页
                        </a>
                        
                        <a class="menu-item" href="/post/about">
                            关于我
                        </a>
                        
                        <a class="menu-item" href="/archives">
                            文章
                        </a>
                        
                        <a class="menu-item" href="/tags">
                            标签分类
                        </a>
                        
                </div>
            </div>
        </nav>
</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementById("menu-toggle-en");
        var toggleMenu_zh = document.getElementById("menu-toggle-zh");

        var mobileMenu = document.getElementById("mobile-menu");
        if (toggleMenu.display != "none") {
            if (toggleMenu.classList.contains("active")) {
                toggleMenu.classList.remove("active")
                mobileMenu.classList.remove("active")
            } else {
                toggleMenu.classList.add("active")
                mobileMenu.classList.add("active")
            }
        } else if (toggleMenu_zh.display != "none") {
            if (toggleMenu_zh.classList.contains("active")) {
                toggleMenu_zh.classList.remove("active")
                mobileMenu.classList.remove("active")
            } else {
                toggleMenu_zh.classList.add("active")
                mobileMenu.classList.add("active")
            }
        }

    }
</script>



    <script>
        (function(a, h, g, f, e, d, c, b) {
            b = function() {
                d = h.createElement(g);
                c = h.getElementsByTagName(g)[0];
                d.src = e;
                d.charset = "utf-8";
                d.async = 1;
                c.parentNode.insertBefore(d, c)
            };
            a["SeniverseWeatherWidgetObject"] = f;
            a[f] || (a[f] = function() {
                (a[f].q = a[f].q || []).push(arguments)
            });
            a[f].l = +new Date();
            if (a.attachEvent) {
                a.attachEvent("onload", b)
            } else {
                a.addEventListener("load", b, false)
            }
        }(window, document, "script", "SeniverseWeatherWidget", "//cdn.sencdn.com/widget2/static/js/bundle.js?t=" + parseInt((new Date().getTime() / 100000000).toString(), 10)));
        window.SeniverseWeatherWidget('show', {
            flavor: "slim",
            location: "WWEFQFPJZ7T8",
            geolocation: true,
            language: "auto",
            unit: "c",
            theme: "auto",
            token: "61bcc333-3305-4728-9465-6785274bb0a3",
            hover: "enabled",
            container: "tp-weather-widget"
        })
    </script>
    
            <div class="main">
                <div class="container">
                    <article class="post-wrap">
                        <header class="post-header">
                            <h1 class="post-title">
                                从attention到GPT
                            </h1>
                            
                                    <!--zh-->
                                    <div class="post-meta zh">
                                        &#x4F5C;&#x8005;:
                                        <a itemprop="author" rel="author" href="/">
                                            Lost Blog
                                        </a>
                                        <span class="post-time">&#x65E5;&#x671F;: <a href="#">2024-01-28</a></span>
                                        <span class="post-readtime">&#x9605;&#x8BFB;&#x65F6;&#x95F4;:<a
                                    href="#">31.2
                                    &#x5206;&#x949F;</a></span>
                                        <span class="post-words">&#x5B57;&#x6570;:<a href="#">8199</a></span>
                                        
                                            <span class="post-category">&#x5206;&#x7C7B;:
                                
                                <a href="https://lostlau.github.io/tag/llm/">大语言模型</a>
                                
                                <a href="https://lostlau.github.io/tag/BRHDZ2fgbY/">transformer</a>
                                
                            </span>
                                            
                                                阅读量:
                                                <span data-hk-page="current"> - </span>
                                                
                                    </div>
                                    
                        </header>
                        
                            <img class="post-feature-image" src="https://lostlau.github.io/post-images/attention_to_gpt.png" alt="">
                          
                        <div class="post-content">
                            <p>去年搞了半年的大语言模型的应用开发。大语言模型的应用技术门槛相对较低，不过发挥第一性原理，抽点时间再回顾一下跟模型架构相关的理论知识。<br>
简略的一些纪要，不会做详细的讲解。</p>
<!-- more -->
<h2 id="0故事线">0.故事线</h2>
<p>现在开源大模型的框架（Llama、GLM、Qwen等）大多是基于GPT的变种，所以GPT的架构到底长什么样？虽然GPT脱颖而出了，但是其同时期的经典框架和它的区别又是什么？要知道这些，需要往下看一层：transformer（因为这些都是基于transformer的上层建筑），所以transformer又长什么样子？我们再往下看，transformer里最核心的结构是attention，它长什么样子？优势又是什么？同时transformer又是一种seq2seq架构，那么什么又是seq2seq？基于这些追问，我们反过来重头开始看。</p>
<h2 id="1-seq2seq">1. Seq2Seq</h2>
<p>Seq2Seq（Sequence to Sequence）是一种在自然语言处理（NLP）领域常用的模型架构，尤其在机器翻译、文本摘要、问答系统和对话系统中非常流行。Seq2Seq模型的核心目标是将一个序列转换为另一个序列，这两个序列的长度可以不同，这使得它非常适用于上述任务。该架构最初由Ilya Sutskever, Oriol Vinyals和Quoc V. Le在2014年提出（<a href="https://arxiv.org/abs/1409.3215">原文arxiv地址</a>），主要目的是解决当时DNN不能很好的处理输出长度不一致的监督学习任务。该架构定义两个核心组成部分：编码器和解码器。</p>
<ul>
<li>编码器（encoder）：编码器的任务是接受输入序列，并将其转换为固定大小的上下文向量（或称为“状态向量”）。这个上下文向量被认为是输入序列的内部表示，包含了输入数据的关键信息。</li>
<li>解码器（decoder）：解码器的任务是接受编码器产生的上下文向量，并基于这个向量生成输出序列。在开始生成过程时，解码器通常接受一个特殊的起始符号，以指示开始生成序列。然后，它逐步生成输出序列中的每个元素，直到遇到特殊的结束符号(&lt;EOS&gt;)，表示序列生成完成。</li>
</ul>
<p>可以想像成encoder主要做表示学习，把原始input表示成高维抽象特征并且学习其空间分布（固定维度）；然后decoder进行自回归生成（采用beam search，解释见下），基于encoder的表征做预测。Ilya的原文中encoder和decoder都是用的LSTM。同时，原文还用了将input倒置的trick（比如 正常句子是abc，但是先颠倒成cba作为模型的input），目的是减少长期依赖的距离。后来有了attention的使用，解决了传统RNN的长时记忆问题，所以这种trick现在基本不使用了。<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/seq2seq.png" alt="" width="70%" loading="lazy"></p>
<h3 id="beam-search">【beam search】</h3>
<p>Beam Search 是一种启发式图搜索算法，常用于自然语言处理中的序列生成任务，特别是在使用序列到序列（Seq2Seq）模型进行机器翻译、文本摘要、语音识别等任务时。与简单的贪心搜索不同，它在每一步并不只是选择概率最高的一个词，而是保留多个最有可能的候选选项，这些候选选项被称为beam。比如目标是最大化：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="double-struck">P</mi><mo>(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><msup><mi>T</mi><msup><mrow></mrow><mo mathvariant="normal">′</mo></msup></msup></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>T</mi></msub><mo>)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>T</mi><msup><mrow></mrow><mo mathvariant="normal">′</mo></msup></msup></munderover><mi mathvariant="double-struck">P</mi><mo>(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><mi>v</mi><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathbb{P}(y_1,\dots,y_{T^{&#x27;}}|x_1,\dots,x_T)=\prod_{t=1}^{T^{&#x27;}} \mathbb{P}(y_t|v,y_1,\dots,y_{t-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.03018em;vertical-align:-0.28018em;"></span><span class="mord"><span class="mord mathbb">P</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.41982em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8928285714285715em;"><span style="top:-2.8928285714285713em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.6068285714285713em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8495600000000001em;"><span style="top:-2.84956em;margin-right:0.1em;"><span class="pstrut" style="height:2.55556em;"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.28018em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.398598em;vertical-align:-1.267113em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.131485em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1164em;"><span style="top:-3.1164em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.6854em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9595600000000001em;"><span style="top:-2.9595599999999997em;margin-right:0.1em;"><span class="pstrut" style="height:2.55556em;"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathbb">P</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span></span>是当前的表征空间（状态特征）。为了最大化左式，可以采用贪心算法（也是beam=1的情况），即在右式中每一个t的时候，都取当前条件概率最大的那个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。但是这样很可能只能得到局部最优，但是计算效率较高。在此基础上我们可以牺牲一点计算效率，然后最优更优的可能性：比如在每一步t，保留可能组合中的top2高的条件概率对应的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>y</mi><mi>t</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">y_t^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2905559999999998em;vertical-align:-0.24575599999999992em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999992em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>y</mi><mi>t</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">y_t^{(2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2905559999999998em;vertical-align:-0.24575599999999992em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.454244em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.2197999999999998em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999992em;"><span></span></span></span></span></span></span></span></span></span>，直到&lt;EOS&gt;(停止符，end of sentences)这便是beam=2的情况。相同的，可以每一步都保留n个当前概率最高的可能情况，就是beam=n。所以beam趋于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span>的时候，就是全空间搜索，必定得到全局最优，但是相应的计算效率就很低了。</p>
<h2 id="2-attention">2. Attention</h2>
<p>通俗的说，attention机制是模拟人类注意力聚焦特性而设计的一种计算单元，它允许模型在处理数据时“关注”到序列的某些部分更多一些，而不是平等的对待每个元素。设想你阅读一本书你的大脑并不是平等地关注每一个单词。相反，你会根据当前句子的上下文和你想要理解的内容，集中注意力于某些词语上。在传统的序列处理模型中，模型是将所有信息压缩进一个固定大小的向量中，不管序列有多长，这就好比要你在听完一个长故事后，只用一个句子来复述它，这显然很难做到既准确又全面。而attention允许模型在生成每个输出时“回头看”输入序列的不同部分，从而根据需要关注更相关的信息。这就像在复述故事的每个部分时，都能重新参考原文，确保你的复述既准确又相关。</p>
<p>从技术层面来讲，注意力机制可以被看作是一个加权求和的过程，其中加权系数决定了在生成输出时对输入序列的不同部分的“关注”程度。这个机制通常被集成到序列处理模型中，不止transformer，像RNN、LSTM也都能使用这个。注意力机制的核心是一个可学习的权重分配过程，这个过程会为输入序列中的每个元素（如单词或字符）分配一个权重，表明每个元素对当前输出元素（如翻译中的单词）的重要性。这些权重随后被用来计算加权平均的上下文向量，该向量将被用作生成当前输出的额外输入。<br>
假设我们有一个输入序列<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>=</mo><mo>{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">X=\{x_1,x_2,\dots,x_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>和一个当前的query（比如decoder中生成的上一状态或输出），attention的计算可以分为以下几步：</p>
<ol>
<li><strong>query, key, value</strong>：首先，对于序列中的每个元素<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，我们通过可学习的参数转换它们为key(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>k</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">k_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）和value（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）。同时，当前的query（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span></span>）也通过相似的变换得到。</li>
<li><strong>注意力得分的计算</strong> ：然后，我们计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span></span>与每个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>k</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">k_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>之间的相似度，即注意力得分<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>k</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">a(q,k_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">a</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。这个得分经典的通过点积得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>q</mi><mi mathvariant="normal">⊤</mi></msup><msub><mi>k</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><msqrt><mi>d</mi></msqrt></mrow><annotation encoding="application/x-tex">q^{\top}k_i/\sqrt{d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.18222em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.93222em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathdefault">d</span></span></span><span style="top:-2.89222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.10777999999999999em;"><span></span></span></span></span></span></span></span></span>（其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>是query的维度）。不过注意力得分也有其他的设计，比如加性得分<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>q</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">tanh(q+k_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>等。</li>
<li><strong>归一化</strong>：注意力分数通过softmax函数进行归一化，以确保所有的权重加起来等于1。这些归一化的分数称为注意力权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</li>
</ol>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo>(</mo><mi>a</mi><mo>(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>k</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>exp</mi><mo>⁡</mo><mo>(</mo><mi>a</mi><mo>(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>k</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">a_i=\frac{\exp(a(q,k_i))}{\sum_{j=1}^n\exp(a(q,k_i))}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.55711em;vertical-align:-1.1301100000000002em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.305708em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1301100000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ol start="4">
<li><strong>加权平均</strong>：最后，计算value（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo separator="true">,</mo><mi mathvariant="normal">∀</mi><mi>i</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">v_i, \forall i=1,\dots,n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∀</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span></span></span></span>）的加权平均，权重为对应的注意力权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">a_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。得到的加权和被称为上下文向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span></span></span></span>，它是输入序列的加权表示，突出了对当前输出最重要的部分。</li>
</ol>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>a</mi><mi>i</mi></msub><mo separator="true">⋅</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c=\sum_{i=1}^n a_i·v_i
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>下面左图是通用的attention机制，右图是经典的点积attention。</p>
<center>
<table>
<thead>
<tr>
<th><img src="https://lostlau.github.io/post-images/attention_to_gpt/attention_general.png"></th>
<th><img src="https://lostlau.github.io/post-images/attention_to_gpt/attention_dot_prod.png" alt="" loading="lazy"></th>
</tr>
</thead>
</table>
</center>
<p>attention里query（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span></span></span></span>）、key（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span></span></span></span>）和value（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>）可以是完全不同的3个特征向量。对某个query（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span></span>）进行相似度查询，查询对象列表就是整个key列表<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span></span></span></span>，检索到相关的key后映射到其value，然后做一个聚合（加权求和）。有点像比如菜市场里所有菜的名字和价格构成了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>，然后你这次想做一个番茄炒蛋，需要买菜，那么你带着这个目的去菜市场，番茄炒蛋就是本次的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span></span>。先q和K进行相似度检索，那么番茄炒蛋最需要的是番茄、鸡蛋，这两个权重很大（相似度得分高）；然后还需要可能洋葱、小葱或者其他的配菜（相似度得分稍低）；那么对于菜市场里有的其他的比如猪肉、鱼这种完全用不到的，在这次买菜（query）里相似度得分就接近0。相似度检索完后找到需要买的菜的价格（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>），然后按权重计算本次买菜行为需要的成本就可以。</p>
<h3 id="self-attention">【self-attention】</h3>
<p>attention机制中有一个很特殊的情况，就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>=</mo><mi>K</mi><mo>=</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q=K=V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>，一个输入同时承担了三种角色，这就是自注意力机制。在NLP任务中，所说的attention基本都是self-attention，因为这些任务（比如机器翻译）中一般输入都只有一个东西，就是句子转换成的词向量组。它们自己和自己玩儿，自我探索结构词和句子语义间的关系😄。</p>
<h3 id="multi-head-attention">【multi-head attention】</h3>
<p>简单的讲就是多个attention的联合。在实际中，给定一组QKV，我们肯定通过注意机制可以捕捉更多的知识，例如捕获这个序列中各种范围的依赖关系（较短范围与较长范围）。因此，直觉上让注意力机制联合使用QKV的不同表示子空间可能是更好的方式，就是说创建多个attention，然后最后再concat起来。<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/multi_head_attention.png" alt="" width="50%" loading="lazy"><br>
在实际操作中，multi-head attention的实现一般会使用一个大的组合权重张量来代表所有的head，执行一次大的矩阵乘法，然后将其分割成每个head的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q,K,V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>矩阵。从理论上讲，这不影响模型输出，因为代数运算是相同的。</p>
<h2 id="3transformer">3.transformer</h2>
<p>最原始的Seq2Seq还是基于RNN的，其不足：</p>
<ol>
<li>encoding过程中长距离衰减问题，当句子过长，越往后的hidden state蕴含的前面的信息就会越弱。</li>
<li>同样的，decoding过程也是一样的长距离衰减问题</li>
<li>解码阶段缺乏对编码阶段各个词的直接利用。简单说就是：机器翻译领域，解码阶段的词和编码阶段的词有很强的映射关系，比如“爱”和“love”。但是seq2seq模型无法再译“love”时直接使用“爱”这个词的信息，因为在编码阶段只能使用全局信息。</li>
</ol>
<p>attention的提出极大的改善了上面的问题，不过attention有个问题是没考虑顺序（这个带来另一个优势就是可以并行，不用像RNN只能串行）。</p>
<p>2017年Google发布了transformer架构，基于Seq2Seq（Encoder-Decoder），并且把RNN踢出了局，改为只用（Self-）Attention+FFNN（前馈神经网络）作为encoder、decoder的核心。所以这篇文章叫：<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all your need</a>。</p>
<p>上一张经典图来展示其架构：<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/transformer.png" alt="" width="100%" loading="lazy"></p>
<p>其序列位置信息的纳入是通过positional encoding的方式来进行的（解释见下面对应部分）。左边灰色框（self-attention + FFNN）是encoder，右边灰色框（masked self-attention + cross attention + FFNN）是decoder。原始transformer里encoder和decoder分别要堆叠6层（有点类似于从单层NN到深度NN，6只是个超参数）。encoder是一个AE（autoencoding）模型，decoder是一个AR（autoregressive）模型。encoder进行的是上下文的理解，复杂特征空间表示学习；而相应的decoder进行一个一个按顺序的预测生成。所以可以看到encoder都是self-attention，到了decoder里的第二层是cross-attention，就是把encoder里学到的表示作为key和value，然后将第一层里的结果作为query。decoder里第一层里使用了mask掩码技巧（解释见下面对应部分）。</p>
<h3 id="positional-encoding">【positional encoding】</h3>
<p>原文中用的是这种方法，使用函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>:</mo><mi mathvariant="double-struck">N</mi><mo>→</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">f:\mathbb{N} \rightarrow \mathbb{R}^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68889em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">N</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span>，直接将位置索引值映射到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>维向量上。公式为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>p</mi><mo separator="true">,</mo><mi>i</mi><mo>)</mo><mo>=</mo><msub><mn mathvariant="bold">1</mn><mrow><mo>{</mo><mi>i</mi><mi mathvariant="normal">为</mi><mi mathvariant="normal">偶</mi><mi mathvariant="normal">数</mi><mo>}</mo></mrow></msub><mo separator="true">⋅</mo><mi>sin</mi><mo>⁡</mo><mo>(</mo><mfrac><mi>p</mi><mrow><mn>1000</mn><msup><mn>0</mn><mfrac><mi>i</mi><mi>d</mi></mfrac></msup></mrow></mfrac><mo>)</mo><mo>+</mo><msub><mn mathvariant="bold">1</mn><mrow><mo>{</mo><mi>i</mi><mi mathvariant="normal">为</mi><mi mathvariant="normal">奇</mi><mi mathvariant="normal">数</mi><mo>}</mo></mrow></msub><mo separator="true">⋅</mo><mi>cos</mi><mo>⁡</mo><mo>(</mo><mfrac><mi>p</mi><mrow><mn>1000</mn><msup><mn>0</mn><mfrac><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow><mi>d</mi></mfrac></msup></mrow></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">f(p,i)=\bold{1}_{\{i为偶数\}}·\sin(\frac{p}{10000^{\frac{i}{d}}})+\bold{1}_{\{i为奇数\}}·\cos(\frac{p}{10000^{\frac{i-1}{d}}})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.94467em;vertical-align:-0.8371099999999999em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">1</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">{</span><span class="mord mathdefault mtight">i</span><span class="mord cjk_fallback mtight">为</span><span class="mord cjk_fallback mtight">偶</span><span class="mord cjk_fallback mtight">数</span><span class="mclose mtight">}</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1075599999999999em;"><span style="top:-2.16289em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9471099999999999em;"><span style="top:-3.3485500000000004em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8550857142857142em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8371099999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.94467em;vertical-align:-0.8371099999999999em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">1</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">{</span><span class="mord mathdefault mtight">i</span><span class="mord cjk_fallback mtight">为</span><span class="mord cjk_fallback mtight">奇</span><span class="mord cjk_fallback mtight">数</span><span class="mclose mtight">}</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1075599999999999em;"><span style="top:-2.16289em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9471099999999999em;"><span style="top:-3.3485500000000004em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8550857142857142em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span style="top:-3.2255000000000003em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8371099999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>为序列中位置索引值，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn><mo>≤</mo><mi>i</mi><mo>&lt;</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">0\leq i&lt;d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.78041em;vertical-align:-0.13597em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69862em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">d</span></span></span></span>是位置编码向量中的位置索引。比如在原序列中的第2个位置就被编码成（假设位置向量是10维）：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mi>f</mi><mo>(</mo><mn>2</mn><mo separator="true">,</mo><mn>0</mn><mo>)</mo><mo separator="true">,</mo><mi>f</mi><mo>(</mo><mn>2</mn><mo separator="true">,</mo><mn>1</mn><mo>)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>f</mi><mo>(</mo><mn>2</mn><mo separator="true">,</mo><mn>9</mn><mo>)</mo><mo>]</mo></mrow><annotation encoding="application/x-tex">[f(2,0), f(2,1), \dots, f(2,9)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">9</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></p>
<p>除了上述的编码方法，还可以使用embedding的手段，就是说像word embedding一样，为序列中每个绝对位置也赋予一个连续、低维、稠密的向量表示。</p>
<h3 id="ffnn">【FFNN】</h3>
<p>这儿前馈神经网络由两个线性变换（全连接层）和一个非线性激活函数组成：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">N</mi></mrow><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo>×</mo><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo>)</mo><mo>×</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathrm{FFN}(x) = \max(0, x\times W_1+b_1) \times W_2 + b_2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">F</span><span class="mord mathrm">F</span><span class="mord mathrm">N</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<h3 id="masked-self-attention">【masked self-attention】</h3>
<p>在decoder里的第一层，目的是在训练的时候不使用未来信息，因为attention可以并行，训练时可以同时进行query和key的点积计算，不过在一句话中当前位置的词query只能跟该位置之前的词key进行注意力得分计算（而在encoder里也可以和这个位置之后的词进行，因为encoder是在做语义的理解）。之后mask的技巧也衍生出不同的掩码方式，这种最原始只向左看齐就是causal mask，此外还有prefix mask等。</p>
<h2 id="4pre-trained-models">4.Pre-trained Models</h2>
<p>经典的NLP领域预训练模型也不是照搬的transformer结构，参考<a href="https://aman.ai/primers/ai/autoregressive-vs-autoencoder-models/">这篇blog</a>分为了以下3种流派：encoder-only（AE），decoder-only（AR）和encoder-decoder（Seq2Seq）。见下图：<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/ae_ar.png" alt="" width="60%" loading="lazy"></p>
<p>decoder-only优缺点：</p>
<ul>
<li>优势：
<ul>
<li>适合生成式任务。因为AR模型使用causal attention去预测下一个token，训练目标也更多以预测下一个词为主，所以天然的就更适合生成任务。</li>
</ul>
</li>
<li>劣势：
<ul>
<li>只能适用于单向（正向或者逆向）上/下文预测任务，不适用于需要双向上下文的任务。</li>
</ul>
</li>
</ul>
<p>encoder-only优缺点：<br>
AE模型更多是用于内容理解的任务，比如经典的BERT（填补缺失句子中的词信息+预测两个句子间是不是前后关系）。</p>
<ul>
<li>优势：
<ul>
<li>可以同时用到上下文的信息，对词与词，句子与句子间的理解捕捉得更好。</li>
</ul>
</li>
<li>劣势：
<ul>
<li>训练中会用到掩码标记<code>[mask]</code>，然而在真实的预测任务中不存在</li>
<li>独立性假设，假设了<code>[mask]</code>和<code>[mask]</code>之间是相互独立的，这个和真实的情况也是有偏差的（同一个句子中这个mask已知之后会改变下一个mask预测的条件分布）</li>
</ul>
</li>
</ul>
<p>encoder-decoder：<br>
同时兼具了AE和AR模型，这种通常用于处理一些需要在输入和输出间建立精确映射的任务，比如机器翻译、文本摘要等。在这些任务中，理解输入的精确内容并据此生成特定的输出是非常重要的。而基于这种架构训练出来的模型，一般只能应用于某种特定的任务。</p>
<p>从这3种架构流派看，encoder-only主打一个理解，decoder主打生成，encoder-decoder从理论上讲应该是最完美的。但是从实践来看，当前主流的LLM都是decoder-only框架，其原因也给一些主观的观点：</p>
<ol>
<li>decoder-only更“知行合一”，他的训练和预测的任务是一致的，更容易往“one for all”的去走，无论什么下游任务，都变成根据指令的生成任务（根据指令的自回归的next word prediction），通过SFT+RLHF来训练。</li>
<li>encoder-only本身设计就是要搭配fine-tuning的，没办法很好的one for all（多个下游模型的fusion空间的对齐也很困难，至少对于当前来说成本很大）。encoder-decoder的精确度是最高的，同时也带来的弊端就是它只适用于specific的任务，它需要被泛化到通用的one for all的代价就更高。</li>
<li>想说timing这个点，其实也是成本导向，decoder-only逻辑上就是最容易先出成果的方向，其到one for all程度的应用实践所需的训练数据的获取、处理、训练时间都比encoder+FT框架更容易、更快。所以当前看decoder-only框架好像更好，可能也只是当下，decoder-only进入瓶颈期后说不定encoder类的框架也会卷出新的高度。当然也可能是跳出transformer的框架。世界总不缺创新的探险者。</li>
</ol>
<h3 id="causal-attention">【causal attention】</h3>
<p>causal的意思是因果(掩码)，也就是每个tokens只能关注到自己和自己左侧的tokens。因果两个字就说明了只能从前到后，意思是第二个字的表示应该包含第一个字的贡献，相反，第一个字的表示，不应该受到第二个字的影响，这就是因果，只有过去可以影响现在，现在不能再对过去产生影响。causal attention也就是带因果掩码的attention机制。</p>
<p>还有一种是prefix attention，其实也是causal的，相当于是对于已经吐出来的那部分subsentence已经是可以相互之间捕捉相关性的了（比如下图中的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">x_1,x_2,x_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）。</p>
<p>与这些相对的是全部都可视的，一般在encoder里存在，相当于做语义理解，捕捉词与词，词与句子的关系，做这些表示学习的时候都是全可见的。</p>
<p>见下图（来自于paper：<a href="https://jmlr.org/papers/volume21/20-074/20-074.pdf">Exploring the Limits of Transfer Learning with a Unified<br>
Text-to-Text Transformer</a>）：<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/causal_prefix.png" alt="" width="90%" loading="lazy"></p>
<h3 id="41-bert">4.1. BERT</h3>
<p>pass</p>
<h3 id="42-gpt">4.2. GPT</h3>
<p>具体可以看<a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html">这篇blog</a>，讲得很好。我就只总结以下重点。</p>
<ol>
<li>GPT3使用了Sparse Attention（见下），提高计算效率。</li>
<li>自从GPT2开始，decoder的layer normalization位置提前了，原生的layer normalization在每次Attention 或者 FeedForward之后，GPT2提到了之前。然后在最后一次decoder之后，进入最后的Linear+Softmax之前增加了一词layer noramlization。</li>
<li>decoder有12个。</li>
<li>每个decoder里multi-head attention有96个head。</li>
</ol>
<p>最后贴一张blog里的手绘架构图，很强（可能看不清楚，建议下下来看或者去原文里看）。<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/GPT_architecture.png" alt="" loading="lazy"></p>
<h3 id="sparse-attention">【Sparse Attention】</h3>
<p>最原始的GPT使用了sparse attention，这个是将attention进行了一些改造，主要是提高计算效率（会降低一些理论精度）。大概看一下我之前的<a href="https://lostlau.github.io/post/transformer_based_ts/">这篇</a>也有讲过这个。</p>
<p>推荐一篇很好的可视化blog：<a href="https://bbycroft.net/llm">LLM visualization</a>，用其中nano-gpt的例子很好的可视化讲解了类GPT的核心结构。</p>
<h2 id="5-llm">5. LLM</h2>
<p>回到大语言模型上，截止目前，大部分的LLM都是采用decoder-only的transformer架构基础，然后在这个基础上做一些变化。这儿再展开讲一下decoder-only的核心架构，它是在原始transformer架构上，移除了以下两个部分：</p>
<ul>
<li>整个encoder部分</li>
<li>decoder中的encoder-decoder self-attention层。</li>
</ul>
<center>
<table>
<thead>
<tr>
<th><img src="https://lostlau.github.io/post-images/attention_to_gpt/decoder-only.png"></th>
<th><img src="https://lostlau.github.io/post-images/attention_to_gpt/decoder-only2.png" alt="" loading="lazy"></th>
</tr>
</thead>
</table>
</center>
<p>大语言模型（或者更广义的基础模型）的提出动机来自于两个事实：</p>
<ul>
<li>大量丰富的非标注语料</li>
<li>标注数据的稀缺性</li>
</ul>
<p>同时，传统大多数深度学习模型，需要大量的标注数据去解决一个表现良好的判别任务，表现得更显一个“局部领域的专家”。它们有一些很难被逾越的缺陷：</p>
<ol>
<li>一些领域没有足够的标注数据。</li>
<li>每次新任务都必须重新建模，成本高。</li>
</ol>
<p>与其每次去训练一个新模型，不如先预训练一个语言模型，然后在此模型的基础上通过一些方式去适配解决各种下游的语言任务。一种方式就是fine-tuning在更多数据上训练一个适合下游任务的模型；另一种更突破性的，就是使用零样本（zero-shot）/少样本（few-shot）推理。</p>
<p>零样本/少样本推理的过程被叫做prompting，其实就是通过指令让语言模型给出回答（这些指令被叫做prompt）。在训练GPT类模型的时候，其接收到的文本数据input一般都是类似于如下样子：</p>
<ul>
<li>&quot;Translate this sentence to English: <sentence> =&gt;&quot;</li>
<li>&quot;Summarize the following document: <document> =&gt;&quot;</li>
</ul>
<p>这些以解决任务的指令input在大规模语料的学习下拥有了zero-shot（在没看过正确output的前提下也能进行作答）/few-shot（给定一些指令和对应指令的正确回答的例子然后进行作答）推理的能力。</p>
<p>要较好的实现以上的zero-shot/few-shot的能力，除了架构和数据，还有工程化很重要的一点：训练方式或者训练步骤。</p>
<ol>
<li>Tokenization</li>
<li>Text Embedding</li>
<li>Pre-training</li>
<li>supervised fine-tuning（SFT）</li>
<li>reinforcement learning with human feedba（RLHF）</li>
</ol>
<p>下面对一些经典的LLM家族的架构进行说明，主要以推理视角，所以以架构为主；训练视角的元素，如训练方式和训练数据如无需要不做特别的说明。（<strong><font color=#e54b4b size=3> 有时间将会持续更新。</font></strong>）</p>
<h3 id="51-llama">5.1. Llama</h3>
<p>Llama基本采用了传统decoder-ony transformer的架构，但在以下3个地方进行的修改：</p>
<ol>
<li><strong>Normalization</strong>。layer normaliztion改成了root mean square normalization（RMSNorm for short），这是一个简化版的LayerNorm，就是将LayerNorm里减去均值的操作省略了（所有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">x_i-\bar{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.56778em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">ˉ</span></span></span></span></span></span></span></span></span>的地方都变成了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>），同时还将线性转换中的需要学习的偏置项取消掉，公式：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><msub><mi>x</mi><mi>i</mi></msub><msqrt><mrow><msup><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mo>×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">y = \frac{x_i}{\sqrt{\bar{x}^2+\epsilon}}\times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.249492em;vertical-align:-0.538em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7114919999999999em;"><span style="top:-2.5445179999999996em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9221171428571429em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.56778em;"><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="mord mtight"><span class="mord mathdefault mtight">x</span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mtight">ˉ</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">ϵ</span></span></span><span style="top:-2.882117142857143em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.11788285714285718em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span>。实验验证以上简化后速度提升40%，效果几乎无损。此外，Llama还采用了pre-nromalization的设计，如GPT2里使用的一样，将normalization处理时机提前，如下图：<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/pre-normalization.png" alt="" width="60%" loading="lazy"></li>
<li><strong>Activation Function</strong>。Llama采用了SwiGLU替代传统的ReLU作为Feed Forward层的激活函数，这个激活函数使用门控单元，里面的包含了2个线性变换，所涉及的权重也是需要学习的，这是和一般的激活函数不同的地方。</li>
</ol>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mi mathvariant="monospace">S</mi><mi mathvariant="monospace">w</mi><mi mathvariant="monospace">i</mi><mi mathvariant="monospace">G</mi><mi mathvariant="monospace">L</mi><mi mathvariant="monospace">U</mi></mrow><mo>=</mo><mrow><mi mathvariant="monospace">S</mi><mi mathvariant="monospace">w</mi><mi mathvariant="monospace">i</mi><mi mathvariant="monospace">s</mi><mi mathvariant="monospace">h</mi></mrow><mo>(</mo><mi>x</mi><msub><mi>W</mi><mi>g</mi></msub><mo>+</mo><msub><mi>b</mi><mi>g</mi></msub><mo>)</mo><mo>⊗</mo><mi>x</mi><mo>(</mo><mi>W</mi><mo>+</mo><mi>b</mi><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mi mathvariant="monospace">S</mi><mi mathvariant="monospace">w</mi><mi mathvariant="monospace">i</mi><mi mathvariant="monospace">s</mi><mi mathvariant="monospace">h</mi></mrow><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>x</mi><mo>⊗</mo><mrow><mi mathvariant="monospace">S</mi><mi mathvariant="monospace">i</mi><mi mathvariant="monospace">g</mi><mi mathvariant="monospace">m</mi><mi mathvariant="monospace">o</mi><mi mathvariant="monospace">i</mi><mi mathvariant="monospace">d</mi></mrow><mo>(</mo><mi>β</mi><mi>x</mi><mo>)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
&amp;\mathtt{SwiGLU}=\mathtt{Swish}(xW_g+b_g)\otimes x(W+b) \\
&amp;\mathtt{Swish}(x)=x\otimes \mathtt{Sigmoid}(\beta x) \\
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.0000000000000004em;vertical-align:-1.2500000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7500000000000002em;"><span style="top:-3.75em;"><span class="pstrut" style="height:2.84em;"></span><span class="mord"></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:2.84em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2500000000000002em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7500000000000002em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathtt">S</span><span class="mord mathtt">w</span><span class="mord mathtt">i</span><span class="mord mathtt">G</span><span class="mord mathtt">L</span><span class="mord mathtt">U</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathtt">S</span><span class="mord mathtt">w</span><span class="mord mathtt">i</span><span class="mord mathtt">s</span><span class="mord mathtt">h</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">b</span><span class="mclose">)</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathtt">S</span><span class="mord mathtt">w</span><span class="mord mathtt">i</span><span class="mord mathtt">s</span><span class="mord mathtt">h</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathtt">S</span><span class="mord mathtt">i</span><span class="mord mathtt">g</span><span class="mord mathtt">m</span><span class="mord mathtt">o</span><span class="mord mathtt">i</span><span class="mord mathtt">d</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2500000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>采用SwiGLU激活的FFN可以表示成（no-bias形式）：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">F</mi><msub><mi mathvariant="normal">N</mi><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">G</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">U</mi></mrow></msub></mrow><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mo>(</mo><msub><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">w</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">h</mi></mrow><mn>1</mn></msub><mo>(</mo><mi>x</mi><mi>W</mi><mo>)</mo><mo>⊗</mo><mi>x</mi><mi>V</mi><mo>)</mo><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathrm{FFN_{SwiGLU}}(x) = (\mathrm{Swish}_1(xW)\otimes xV)W_2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">F</span><span class="mord mathrm">F</span><span class="mord"><span class="mord mathrm">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">S</span><span class="mord mathrm mtight" style="margin-right:0.01389em;">w</span><span class="mord mathrm mtight">i</span><span class="mord mathrm mtight">G</span><span class="mord mathrm mtight">L</span><span class="mord mathrm mtight">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathrm">S</span><span class="mord mathrm" style="margin-right:0.01389em;">w</span><span class="mord mathrm">i</span><span class="mord mathrm">s</span><span class="mord mathrm">h</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">x</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<ol start="3">
<li><strong>RoPE</strong>。在位置编码（positional embedding）上引用Rotary Positional Embedding，是一种结合了绝对位置编码和相对位置编码的技术。具体细节见<a href="https://blog.eleuther.ai/rotary-embeddings/">这篇blog</a>或者<a href="https://zhuanlan.zhihu.com/p/647109286">这篇知乎</a>。使用RoPE可以在长序列上带来更好的位置嵌入效果。需要补充的是，RoPE是和self-attention同时进行的，所以不是跟传统的PE一样在Input Text Embedding的时候进行一次，而是在每个有self-attention的地方都需要运行。</li>
</ol>
<p>在以上Llama的变化基础上，Llama还对架构进行了进一步修改，最关键就是采用了grouped-query attention来替代原始的multi-head attention。grouped-query attention介于multi-head attention和multi-query attention（由<a href="https://arxiv.org/pdf/1911.02150.pdf">这篇paper</a>提出，在PaLM模型里使用并得到较好实践效果）之间，3者关系如下图所示：<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/grouped-query_attention.png" alt="" width="80%" loading="lazy"><br>
其中，multi-query attention的改变是在于在每个head之间共享了相同的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">K,V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>，这使得推理的速度可以得到明显提升（以前<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">K,V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>上计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span></span></span>次的现在只需要1次）。但是提一下，训练速度上并无太大提升，原因是训练更大的开销在于梯度下降的计算和参数更新上。可见，grouped-query attention是将原始的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span></span></span>个multi-head又分成了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo>(</mo><mo>&lt;</mo><mi>H</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">h(&lt;H)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mclose">)</span></span></span></span>个小组，然后在小组间共享<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">K,V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>，相当于是在预测效果和推理速度之间做了一个trade-off。</p>
<p>此外，在多轮对话推理的优化上使用了Ghost Attention技术。</p>
<h3 id="52-falcon">5.2. Falcon</h3>
<p>Falcon依然采用的decoder-only causal transformer 架构。在传统基础上，进行了如下几处修改：</p>
<ol>
<li><strong>Flash Attention</strong>。这是一个可以加速attention计算的算法优化，不过这个算法的核心不是在理论上降低计算复杂度（比如reformer、informer等），而是在硬件上，它重新制定了Attention计算在GPU上不同计算单元间的调度，以此来达到真实场景下的提速。GPU硬件上本人不是太行，对细节感兴趣的可以看<a href="https://shreyansh26.github.io/post/2023-03-26_flash-attention/">这篇blog</a>。实际运用中，FlashAttention可以对推理提一些速（10%-40%），但是效果感觉并不是那么惊人。</li>
<li><strong>RoPE embeddings</strong>。见Llama章节介绍。</li>
<li><strong>Multi-Query Attention</strong>。见Llama章节介绍。</li>
<li><strong>Parallel Attetnion and Feed-Forward layers</strong>。传统串行的attention+feed-forward不同，Falcon采用了并行的方式（我记得最初好像是由PaLM提出来的），相当于对于input，分别进行self-attention和feed-forward，然后再加起来（同时还需要加上原input，因为ResNet）。大概如下图：<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/parallel-decoder-layer.png" alt="" width="60%" loading="lazy"></li>
</ol>
<h3 id="53-chatglm">5.3. ChatGLM</h3>
<p>ChatGLM采用的是GLM架构，是一个挺有意思的架构。它的诞生其实是在LLM井喷之前，在预训练模型被Bert、GPT和T5三分天下的时候。GLM也是采用的decoder-only transformer，但是它与GPT很大的不同在于，它并不只是做NLG（next token prediction），同样的他还要做NLU（Bert的强项）。GLM巧妙的设计了一个机制，autoregressive blank filling，使得它在预训练的时候可以既做理解训练又做生成训练。机制如下（可以对照下面的官方案例图一起看）：</p>
<ol>
<li>对于收集到的某一条语料句子<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>=</mo><mo>{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>3</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>4</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>5</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>6</mn></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">X=\{x_1,x_2,x_3,x_4,x_5,x_6\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>，首先从里面采样子句子（连续token），比如图中的sub-sentence1：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><msub><mi>x</mi><mn>3</mn></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">\{x_3\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>和sub-sentence2：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><msub><mi>x</mi><mn>5</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>6</mn></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">\{x_5,x_6\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>。</li>
<li>将原始句子分为两部分:
<ul>
<li>PartA：挖掉采样子句子后剩下的原句子，并将挖掉的部分留空白用<code>[M]</code>表示。这儿即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>[</mo><mi mathvariant="monospace">M</mi><mo>]</mo><mo separator="true">,</mo><msub><mi>x</mi><mn>4</mn></msub><mo separator="true">,</mo><mo>[</mo><mi mathvariant="monospace">M</mi><mo>]</mo><mo>}</mo></mrow><annotation encoding="application/x-tex">\{x_1,x_2,[\mathtt{M}],x_4,[\mathtt{M}]\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathtt">M</span></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathtt">M</span></span><span class="mclose">]</span><span class="mclose">}</span></span></span></span>。</li>
<li>PartB：刚才采样的子句子，这儿即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><msub><mi>x</mi><mn>3</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>5</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>6</mn></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">\{x_3,x_5,x_6\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>。</li>
</ul>
</li>
<li>重排序列组合生成Input。先将PartB里的子句子shuffle随机排序，比如这儿sub-sentence2就变到了sub-sentence1前面，变成了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><msub><mi>x</mi><mn>5</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>6</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">\{x_5,x_6,x_3\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>。然后加上开始token（<code>[S]</code>）。最后将PartA和当前PartB拼接起来。Input就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>[</mo><mi mathvariant="monospace">M</mi><mo>]</mo><mo separator="true">,</mo><msub><mi>x</mi><mn>4</mn></msub><mo separator="true">,</mo><mo>[</mo><mi mathvariant="monospace">M</mi><mo>]</mo><mo separator="true">,</mo><mo>[</mo><mi mathvariant="monospace">S</mi><mo>]</mo><mo separator="true">,</mo><msub><mi>x</mi><mn>5</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>6</mn></msub><mo separator="true">,</mo><mo>[</mo><mi mathvariant="monospace">S</mi><mo>]</mo><mo separator="true">,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>}</mo></mrow><annotation encoding="application/x-tex">\{x_1,x_2,[\mathtt{M}],x_4,[\mathtt{M}],[\mathtt{S}],x_5,x_6,[\mathtt{S}],x_3\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathtt">M</span></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathtt">M</span></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathtt">S</span></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathtt">S</span></span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>。</li>
<li>给重排序后的序列进行位置编码，使用了2D-positional encoding。第一个位置编码是原始句子中的位置，填空的部分（<code>[M]</code>）用一个位置标记。第二个位置编码是对于PartB的子序列标记其原有的内在顺序。</li>
<li>将上述Input和位置编码放入GLM，进行双目标训练：完形填空（靠PartA）和下文生成（靠PartB）。所以其Attention掩码的设计就不是经典的causal mask，而是prefix mask。PartA是非causal的，相互之间可以看到，进行上下文理解，做完形填空。而PartB里就是causal的，做下文预测。从这儿可以感受一下，PartA就类似Bert，同时PartB的shuffle设计还将不同<code>[M]</code>之间变得非独立（克服了Bert里独立性的假设）。</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://lostlau.github.io/post-images/attention_to_gpt/chatglm.png" alt="" width="100%" loading="lazy"></figure>
<p>除了以上最核心设计的区别，GLM在在原始的transformer架构上还：</p>
<ul>
<li>更改了LayerNorm和ResNet的顺序。改为先LayerNorm后Add。</li>
<li>用单个线性层来进行最终token的预测</li>
<li>激活函数用GeLUs替代ReLU</li>
</ul>
<h3 id="54-qwen">5.4. Qwen</h3>
<p>讲道理，在我已有的实践中，千问是用得比较多的，所以po一下它的家族系列：<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/qwen_series.png" alt="" width="100%" loading="lazy"><br>
目前开源社区里的主要是蓝色和紫色的部分，RLHF部分应该是千问商用API的😄。</p>
<p>看了千问的<a href="https://arxiv.org/pdf/2309.16609.pdf">技术报告</a>，架构上基本同Llama。一些小调整在于：</p>
<ol>
<li>embedding层和output层的权重矩阵分离（传统transformer应该是tied weights，就是这两个是共享的。Llama2里好像没有说这个）</li>
<li>Input到QKV的转换加入了bias（很多是没有加的），即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi><mo>=</mo><mi>I</mi><mo>×</mo><msub><mi>W</mi><mi>q</mi></msub><mo>+</mo><msub><mi>b</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">Q=I \times W_q+b_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">I</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></li>
</ol>
<p>架构之外，在推理上，关于长序列推理加入几个重要的training-free技术：</p>
<ol>
<li><strong>dynamic NTK</strong>。具体可见<a href="https://zhuanlan.zhihu.com/p/670149880">这篇知乎文章</a></li>
<li><strong>LogN-Scaling</strong>。基于熵不变性为出发点的外推技术，推导可以看<a href="https://spaces.ac.cn/archives/8823">这篇blog</a></li>
<li><strong>window attention</strong>。类似于Sparse Attention，不过这儿只是在推理的时候才用到，训练的时候不用。将attention的计算只限定在一个固定窗口长度中。</li>
</ol>
<h3 id="55-mixtral">5.5. Mixtral</h3>
<p>Mixtral的出名是提出了一种新的行之有效（评估结果很好）的架构模组：MoE（Mixture of Experts）。我们先介绍原始的Mixtral-7B，然后再引出Mixtral-MoE（8✖7B）。</p>
<p><a href="https://arxiv.org/pdf/2310.06825.pdf">Mixtral-7B</a>在架构上基本还是采用的Llama架构，在此基础上最核心的改变是使用sliding window attention（SWA，道理同sparse attention）:<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/mixtral_swa.png" alt="" width="70%" loading="lazy">。<br>
由于decoder blog一般都会重复很多次，所以虽然每次当前token的注意力都只在只限定在一个窗口里，但是一层一层传递的时候，这个token会受到较多前文的影响（指数增加，如上图最右边例子）。理论上说是这么一说，原文中window size设置的也只是最长回望上下文长度的一半😄。</p>
<p>推理性能上也做了两个调整，适配SWA：Rolling Buffer Cache和Pre-fill Chunking。都是在cache上的调整，具体可以见原文。</p>
<p>Mistral-7B架构上感觉很普通，但是也有不错的评估效果，我认为应该是在数据上做了不少文章。</p>
<p>在Mistral-7B基础上，该团队提出了MoE的模型（Mistral 8*7B），获得了较大的成功。<a href="https://arxiv.org/pdf/2401.04088.pdf">Mistral-MoE</a>提出了一种新的transformer decoder-only架构变种。将decoder从GPT开始一直沿用的[ Attention + Add&amp;Norm -&gt;  FeedForward + Add&amp;Norm ]中的FeedForward layer替换成了MoE layer，借鉴一下下图：<br>
<img src="https://lostlau.github.io/post-images/attention_to_gpt/mixtral_moe.png" alt="" width="100%" loading="lazy">。<br>
MoE首次提出是在20年（<a href="https://arxiv.org/pdf/2006.16668.pdf">这儿</a>），之前没什么名气。这个layer里主要包含两个东西：router和expert。MoE整体实现是个加权求和，router给出了加权的权重，expert给出求和的每个元素。所以，对于给定的Input <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>，有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span>（这是个超参数）个experts的MoE层的output可以表示为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>G</mi><mo>(</mo><mi>x</mi><msub><mo>)</mo><mi>i</mi></msub><mo>⋅</mo><msub><mi>E</mi><mi>i</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\sum_{i=1}^n G(x)_i \cdot E_i(x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.929066em;vertical-align:-1.277669em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></p>
<p>这儿，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mo>(</mo><mi>x</mi><msub><mo>)</mo><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">G(x)_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>表示gating network（即router）的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span>维output中的第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>个（对应第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>个expert的权重），<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>E</mi><mi>i</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">E_i(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>是第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>个expert network的output。</p>
<p>所谓的gating network <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">G(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>是用softmax计算得到的权重，但是这儿的softmax是取的top K（K为超参数）个元素的softmax：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>:</mo><mo>=</mo><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mo>(</mo><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">K</mi></mrow><mo>(</mo><mi>x</mi><mo>⋅</mo><msub><mi>W</mi><mi>g</mi></msub><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">G(x):=\mathrm{Softmax}(\mathrm{TopK}(x\cdot W_g))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">S</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathrm">T</span><span class="mord mathrm">o</span><span class="mord mathrm">p</span><span class="mord mathrm">K</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>相当于x先进行线性转换，在得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68889em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span></span></span></span>的向量里只取最大的前K个来进行softmax，只有前K个有权重（概率），后n-K个权重都为0。比如原文中是8个experts，每次只取前2个来进行softmax。可以想象成每一个入参都由8个专家审核，但是每次只会采纳2个专家的建议。</p>
<p>“专家的建议”<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">E(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>则由SwiGLU激活单元表示（没有过多的解释，很好奇为什么不是SwiGLU的FFN🤔️）。</p>
<p>MoE最核心的就是以上了。整体感觉MoE没有什么复杂的操作，但是却实现了一个比较好的效果，就很“漂亮”。然后虽然名字叫Mixtral-8*7B，但是实际参数量并不是56B，只有47B，因为8个专家只有在expert network上各自独立参数，其他的在attention层、resnet、norm、router、output层都是共享的相同权重矩阵。</p>

                        </div>
                        
                            <div class="post-toc">
                                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#0%E6%95%85%E4%BA%8B%E7%BA%BF">0.故事线</a></li>
<li><a href="#1-seq2seq">1. Seq2Seq</a>
<ul>
<li><a href="#beam-search">【beam search】</a></li>
</ul>
</li>
<li><a href="#2-attention">2. Attention</a>
<ul>
<li><a href="#self-attention">【self-attention】</a></li>
<li><a href="#multi-head-attention">【multi-head attention】</a></li>
</ul>
</li>
<li><a href="#3transformer">3.transformer</a>
<ul>
<li><a href="#positional-encoding">【positional encoding】</a></li>
<li><a href="#ffnn">【FFNN】</a></li>
<li><a href="#masked-self-attention">【masked self-attention】</a></li>
</ul>
</li>
<li><a href="#4pre-trained-models">4.Pre-trained Models</a>
<ul>
<li><a href="#causal-attention">【causal attention】</a></li>
<li><a href="#41-bert">4.1. BERT</a></li>
<li><a href="#42-gpt">4.2. GPT</a></li>
<li><a href="#sparse-attention">【Sparse Attention】</a></li>
</ul>
</li>
<li><a href="#5-llm">5. LLM</a>
<ul>
<li><a href="#51-llama">5.1. Llama</a></li>
<li><a href="#52-falcon">5.2. Falcon</a></li>
<li><a href="#53-chatglm">5.3. ChatGLM</a></li>
<li><a href="#54-qwen">5.4. Qwen</a></li>
<li><a href="#55-mixtral">5.5. Mixtral</a></li>
</ul>
</li>
</ul>
</li>
</ul>

                            </div>
                            
                                
                                    
                                            <!--zh-->
                                            <section class="post-copyright zh">
                                                <p class="copyright-item ">
                                                    <span>&#x4F5C;&#x8005;:</span>
                                                    <span>Lost Blog</span>
                                                </p>

                                                <p class="copyright-item">
                                                    <span>&#x6C38;&#x4E45;&#x94FE;&#x63A5;:</span>
                                                    <span><a href="https://lostlau.github.io/post/attention_to_gpt/">https://lostlau.github.io/post/attention_to_gpt/</a></span>
                                                </p>

                                                <p class="copyright-item">
                                                    <span>&#x534F;&#x8BAE;:</span>
                                                    <span>MIT License</span>
                                                </p>
                                            </section>
                                            
                                                
                                                    
                                                                    <!--Share-->
                                                                    <span style="margin-right:15px">
                        <i class="post-share"></i>
                        <span>&#x5206;&#x4EAB;:</span>
                                                                    <a title="QR 码" target="_blank" href="https://api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://lostlau.github.io/post/attention_to_gpt/"><i class="fa fa-qrcode"></i></a>
                                                                    <a title="QQ" target="_blank" href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://lostlau.github.io/post/attention_to_gpt/&sharesource=qzone&title=从attention到GPT&pics=https://lostlau.github.io/images/avatar.png?v=1753006403576&summary=&lt;p&gt;去年搞了半年的大语言模型的应用开发。大语言模型的应用技术门槛相对较低，不过发挥第一性原理，抽点时间再回顾一下跟模型架构相关的理论知识。&lt;br&gt;
简略的一些纪要，不会做详细的讲解。&lt;/p&gt;
"><i class="fa fa-qq"></i></a>
                                                                    <a title="新浪微博" target="_blank" href="https://service.weibo.com/share/share.php?url=https://lostlau.github.io/post/attention_to_gpt/&sharesource=weibo&title=从attention到GPT + " - " + &lt;p&gt;去年搞了半年的大语言模型的应用开发。大语言模型的应用技术门槛相对较低，不过发挥第一性原理，抽点时间再回顾一下跟模型架构相关的理论知识。&lt;br&gt;
简略的一些纪要，不会做详细的讲解。&lt;/p&gt;
&pic="https://lostlau.github.io/images/avatar.png?v=1753006403576 "><i class="fa fa-weibo "></i></a>

                                                                    </span>
                                                                    
                                                                            <!--zh-->
                                                                            <section class="post-tags zh ">
                                                                                <div>
                                                                                    <span>&#x6807;&#x7B7E;:</span>
                                                                                    <span class="tag ">
                        
                        
                        <a href="https://lostlau.github.io/tag/llm/">#
                    大语言模型
                        </a>
                        
                        <a href="https://lostlau.github.io/tag/BRHDZ2fgbY/">#
                    transformer
                        </a>
                        
                            
                                </span>
                                                                                </div>
                                                                                <div>
                                                                                    <a href="javascript:window.history.back();">
                        &#x8FD4;&#x56DE;</a>
                                                                                    <span>&dot;</span>
                                                                                    <a href="#">&#x4E3B;&#x9875;</a>
                                                                                </div>
                                                                            </section>

                                                                            
                                                                                <!---->
                                                                                <section class="post-nav">
                                                                                    
                                                                                        <a class="prev" rel="prev" href="https://lostlau.github.io/post/intro_reinforcement_learning/">
                                                                                            强化学习的基本概念
                                                                                        </a>
                                                                                        
                                                                                            
                                                                                                <a class="next" rel="next" href="https://lostlau.github.io/post/quantization/">
                                                                                                    数值计算的精度与量化技术
                                                                                                </a>
                                                                                                
                                                                                </section>
                                                                                <br>
                                                                                <br>
                                                                                <br>
                                                                                <br>
                                                                                
                                                                                                            
                                                                                                                <script type="application/javascript" src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
<div id="vlaine-comment"></div>
<script type="application/javascript">
    new Valine({
        el: '#vlaine-comment',
        appId: 'IDwm6MbiTzz7kIRCFR8CK7G4-9Nh9j0Va',
        appKey: 'ghgXCbobWxBYTG3VWr1uTRpF',
        pageSize: 10,
        avatar: 'mp',
        placeholder: '水平一般，能力有限，能回答的博主尽量回答:)',
        visitor: true,
        highlight: true,
        recordIP: false,
    })
</script>
                                                                                                                    
                    </article>
                </div>
            </div>
    </div>
    </div>
    </div>
    
        <footer id="footer" class="footer">
            <div class="copyright">
                
                        Created by <a href="https://github.com/lostlau/lostlau.github.io" target="_blank">Lostlau</a>
                            
                                        <svg viewBox="0 0 1024 1024" style="margin-left: 5px;margin-right: 5px;" version="1.0" width="8" height="8" class="my-face">
            <path
                d="M863.597631 513.574282l-271.33965-140.213484L729.783667 81.926656c3.583731-7.87141 7.167462-15.742819 7.167462-25.214109C736.887134 25.226908 708.345275 0.012799 672.635953 0.012799a63.611229 63.611229 0 0 0-39.293053 12.607055c-1.791866 1.59988-3.519736 3.19976-5.311602 3.19976L147.87531 418.925381a55.547834 55.547834 0 0 0-19.646527 47.356448c1.791866 17.278704 14.27093 33.021523 32.125591 42.492813l271.33965 141.749369L292.504463 945.221908c-12.479064 25.214109-1.791866 53.563983 23.166262 69.306802 10.751194 6.335525 23.230258 9.47129 35.709322 9.47129 16.062795 0 32.125591-4.735645 44.604655-15.742819l480.091993-403.297753a55.547834 55.547834 0 0 0 19.646526-47.228458 61.243407 61.243407 0 0 0-32.12559-44.156688z"
                fill="#93b5cf"></path>
        </svg>
                                        Lost Blog &copy;Copyright
                                            <script>
                                                var date = new Date();
                                                document.write("" + date.getFullYear());
                                            </script>
                                            | Powered by
                                            <a href="https://github.com/ITJoker233/Gridea-theme-Chic" target="_blank">
                                                Chic
                                            </a>
            </div>
            <div id="update" style="display:none;">
                on
            </div>
            
                <div id="version" style="display:none;">
                    1.7.6
                </div>
                
                    <script>
                        var port = '';
                        
                        document.write('<div id="home_path" style="display:none;">' + document.location.protocol + '//' + window.document.location.hostname + port + '</div>')
                    </script>
        </footer>
        
            <script src='https://fastly.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js'></script>
            
                <script>
                    
                    
                    loadlive2d();
                    
                    
                    getStar();
                    hljs.initHighlighting();
                    console.clear();
                    
                    CheckVersion();
                    
                    var newDate = new Date();
                    newDate.setTime(1753006403576);
                    console.log(" Blog Update Time: " + newDate.toLocaleDateString());
                    console.log("\n %c \u26a1Theme:Chic Author's Blog:https://blog.itjoker.cn  Write By ITJoker  \n\n", "color: #ffffff; background: rgba(49, 49, 49, 0.85); padding:5px 0;border-radius:5px;");
                </script>
        </div>
</body>
<script>
    scroll();
</script>

</html>
